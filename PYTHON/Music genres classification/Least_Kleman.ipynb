{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyN7nx3itpDaYXrSTAGLE00N"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","from urllib.request import urlopen\n","from PIL import Image\n","import torch\n","from transformers import AutoFeatureExtractor, ResNetForImageClassification, ResNetModel\n","import os\n","from PIL import Image\n","from torch.utils.data import TensorDataset, DataLoader\n","import time\n","import numpy as np\n","device = 'cuda'\n","from transformers import ResNetModel\n","import torch\n","\n","def train(net, train_dataloader, criterion, optimizer, scheduler=None, epochs=100, device=device, checkpoint_epochs=2, timeout=45):\n","    start = time.time()\n","    print(f'Training for {epochs} epochs on {device}')\n","\n","    for epoch in range(1,epochs+1):\n","        print(f\"Epoch {epoch}/{epochs}\")\n","\n","        net.train()  # put network in train mode for Dropout and Batch Normalization\n","        train_loss = torch.tensor(0., device=device)  # loss and accuracy tensors are on the GPU to avoid data transfers\n","        train_accuracy = torch.tensor(0., device=device)\n","        for X, y in train_dataloader:\n","            X = X.to(device)\n","            y = y.type(torch.LongTensor).to(device)\n","            preds = net(X)\n","            loss = criterion(preds, y)\n","\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","            with torch.no_grad():\n","                train_loss += loss * train_dataloader.batch_size\n","                train_accuracy += (torch.argmax(preds, dim=1) == y).sum()\n","\n","        if scheduler is not None:\n","            scheduler.step()\n","\n","        print(f'Training loss: {train_loss/len(train_dataloader.dataset):.2f}')\n","        print(f'Training accuracy: {100*train_accuracy/len(train_dataloader.dataset):.2f}')\n","\n","\n","        if epoch%checkpoint_epochs==0:\n","            torch.save({\n","                'epoch': epoch,\n","                'state_dict': net.state_dict(),\n","                'optimizer': optimizer.state_dict(),\n","            }, './checkpoint.pth.tar')\n","\n","        print()\n","        if((time.time() - start) >= timeout):\n","          break\n","\n","    end = time.time()\n","    print(f'Total training time: {end-start:.1f} seconds')\n","    return net\n","\n","# model definition\n","class Classifier_model(torch.nn.Module):\n","    # define model elements\n","    def __init__(self):\n","        super(Classifier_model, self).__init__()\n","        self.device = device\n","        self.image_processor = AutoFeatureExtractor.from_pretrained(\"microsoft/resnet-18\",device=self.device)\n","        self.pre_trained_model = ResNetModel.from_pretrained(\"microsoft/resnet-18\")\n","        resnet18_output_size=25088\n","        self.fc = torch.nn.Linear(resnet18_output_size, 10)\n","        self.activation = torch.nn.ReLU()\n","\n","    # forward propagate input\n","    def forward(self, X):\n","        X = self.image_processor(X, return_tensors=\"pt\").to(self.device)\n","        # print(X.pixel_value.is_cuda)\n","        X = self.pre_trained_model(**X).last_hidden_state.flatten(start_dim=1)\n","        X = self.activation(X)\n","        X = self.fc(X)\n","\n","        return X.softmax(dim=1)\n","\n","    def features_extractor(self, X):\n","        X = self.image_processor(X, return_tensors=\"pt\").to(self.device)\n","        # print(X.pixel_value.is_cuda)\n","        X = self.pre_trained_model(**X).last_hidden_state.flatten(start_dim=1)\n","\n","        return X.softmax(dim=1)\n","\n","# model definition\n","class Classifier_model_2(torch.nn.Module):\n","    # define model elements\n","    def __init__(self,f_model):\n","        super(Classifier_model_2, self).__init__()\n","        self.device = device\n","        resnet18_output_size=25088\n","        self.f_model = f_model\n","        self.fc = torch.nn.Linear(resnet18_output_size, 2)\n","        self.activation = torch.nn.ReLU()\n","\n","    # forward propagate input\n","    def forward(self, X):\n","        #X = self.image_processor(X, return_tensors=\"pt\").to(self.device)\n","        # print(X.pixel_value.is_cuda)\n","        X = self.f_model.features_extractor(X)\n","        X = self.activation(X)\n","        X = self.fc(X)\n","\n","        return X.softmax(dim=1)\n","\n","\n","# Chemin du répertoire racine\n","import random\n","root_dir = '/content/drive/MyDrive/ColabNotebooks/mel'\n","\n","# Dictionnaire pour stocker les images par sous-dossier\n","image_dict = {}\n","\n","# Parcourir tous les sous-dossiers\n","for root, dirs, files in os.walk(root_dir):\n","    for file in files:\n","        # Construire le chemin complet du fichier\n","        file_path = os.path.join(root, file)\n","\n","        # Vérifier si le fichier est une image en fonction de l'extension (par exemple, .png)\n","        if file_path.lower().endswith(('.png', '.jpg', '.jpeg', '.gif')):\n","            # Ouvrir l'image avec Pillow\n","            image = Image.open(file_path)\n","\n","            # Convertir l'image en format JPG (si elle n'est pas déjà en JPG)\n","            if image.format != \"JPEG\":\n","                image = image.convert(\"RGB\")\n","\n","            # Obtenez le nom du sous-dossier parent\n","            parent_dir = os.path.basename(os.path.dirname(file_path))\n","\n","            # Vérifiez si le sous-dossier existe dans le dictionnaire, sinon créez-le\n","            if parent_dir not in image_dict:\n","                image_dict[parent_dir] = []\n","\n","            # Ajouter l'image à la liste du sous-dossier correspondant\n","            image = image.resize((336,219))\n","            image_dict[parent_dir].append(np.array(image))\n","\n","classical_list_MG = image_dict['classic']\n","non_classical_list_MG = image_dict['non_classic']\n","\n","percent_classical_MG = int(0.2*len(classical_list_MG))\n","percent_non_classical_MG = int(0.2*len(non_classical_list_MG))\n","\n","\n","evaluation_list_dic = []\n","for sample in classical_list_MG[:percent_classical_MG]:\n","    buffer = []\n","    buffer.append(sample)\n","    buffer.append(0)\n","    evaluation_list_dic.append(buffer)\n","\n","for sample in non_classical_list_MG[:percent_non_classical_MG]:\n","    buffer = []\n","    buffer.append(sample)\n","    buffer.append(1)\n","    evaluation_list_dic.append(buffer)\n","\n","random.shuffle(evaluation_list_dic)\n","evaluation_list = []\n","for sample in evaluation_list_dic:\n","    evaluation_list.append(sample[0])\n","\n","activeL_classical_list = classical_list_MG[percent_classical_MG:]\n","activeL_non_classical_list = non_classical_list_MG[percent_non_classical_MG:]\n"],"metadata":{"id":"09mt6poN8zpx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1702994698037,"user_tz":-60,"elapsed":59133,"user":{"displayName":"Clemzi","userId":"17076433745448136774"}},"outputId":"0434ce46-3d80-4e1c-ca90-37f105d1f4be"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["def LeastDataLoader(percent, previousPercent, uncertainties, dataset):\n","  \"\"\"\n","  This function is used after having determined the Least Confidence of the base dataset.\n","  Keywords:\n","    percent         : The percentage of the base dataset to take account of\n","    previousPercent : The previous percentage that was used the last time this function was called\n","    uncertainties   : A dictionnary containing the uncertainties associated with each sample of dataset\n","    dataset         : The updated dataset containing only unseen samples\n","  Returns:\n","    my_dataloader   : pytorch compatible data corresponding to the dataset used for the model training\n","    dataset         : The new dataset containing unseen samples\n","  \"\"\"\n","\n","  # How many samples do I have to care about this time ?\n","  difference_in_percent = percent - previousPercent\n","  lastPoint_uncertainties = int(difference_in_percent * len(uncertainties))\n","\n","  # Taking only that many samples into consideration\n","  samples_to_take = list(uncertainties.keys())[:lastPoint_uncertainties]\n","  print(samples_to_take[:10])\n","  # the indices inside the dataset of the most uncertained samples\n","  data = []\n","  for i in range(len(samples_to_take)):\n","    data.append(dataset[samples_to_take[i]])\n","\n","  dataset = np.delete(dataset, samples_to_take, axis=0)\n","\n","  # Now, we need to recreate two tensors knowing if each samples contained\n","  # inside the dataset is classical or not.\n","  new_active_classical = []\n","  new_active_non_classical = []\n","  for item in data:\n","      if any(np.array_equal(item, x) for x in activeL_classical_list):\n","          new_active_classical.append(item)\n","      else:\n","          new_active_non_classical.append(item)\n","\n","  if (len(new_active_classical) == 0):\n","    tensor_x = torch.Tensor(new_active_non_classical).to(device)\n","    tensor_y = torch.Tensor(np.full(len(new_active_non_classical),  1)).to(device)\n","\n","\n","  else:\n","    tensor_x = torch.Tensor(np.concatenate((new_active_classical, new_active_non_classical), axis = 0)).to(device)\n","    tensor_y = torch.Tensor(np.concatenate((np.full(len(new_active_classical), 0),\n","                                        np.full(len(new_active_non_classical),  1)), axis=0)).to(device)\n","\n","\n","  my_dataset = TensorDataset(tensor_x,tensor_y) # create your datset\n","  my_dataloader = DataLoader(my_dataset,batch_size=10, shuffle=True)\n","  return my_dataloader, dataset"],"metadata":{"id":"PXpyRHTiD4pX","executionInfo":{"status":"ok","timestamp":1702994698038,"user_tz":-60,"elapsed":67,"user":{"displayName":"Clemzi","userId":"17076433745448136774"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["def LeastConfidence(model, test_data):\n","  \"\"\"\n","  Keywords:\n","    model : The model that gives predictions\n","    data  : The dataset to base the predictions on\n","    n     : the number of best candidates to select\n","\n","  Returns:\n","    A sorted dictionnary containing the uncertainty score associated with a sample\n","  \"\"\"\n","  uncertainty_dict = {}\n","  for i in range(len(test_data)):\n","    preds = model(test_data[i])\n","    uncertainty_dict[i] = 1 - np.max(preds[0].cpu().detach().numpy())\n","  res = dict(sorted(uncertainty_dict.items(),\n","                    key = lambda x: x[1], reverse = True))\n","  for key in list(res.keys())[:10]:\n","      print(f\"{key}: {res[key]}\")\n","  return res"],"metadata":{"id":"rNbQ6GVnD-14","executionInfo":{"status":"ok","timestamp":1702994698039,"user_tz":-60,"elapsed":62,"user":{"displayName":"Clemzi","userId":"17076433745448136774"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","execution_count":4,"metadata":{"id":"St5iHgGYfZCn","executionInfo":{"status":"ok","timestamp":1702994699201,"user_tz":-60,"elapsed":1221,"user":{"displayName":"Clemzi","userId":"17076433745448136774"}}},"outputs":[],"source":["import os\n","import random\n","import numpy as np\n","import pandas as pd\n","import torch\n","from torch.utils.data import DataLoader, TensorDataset\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","\n","def leastConfidence_Train(modelToTrain, evaluation_list,save_dir='/content/drive/MyDrive/ColabNotebooks/models/LeastConfidence'):\n","\n","    # Créer un répertoire s'il n'existe pas pour y stocker les modèles\n","    if not os.path.exists(save_dir):\n","        os.makedirs(save_dir)\n","\n","    results = []\n","\n","    # Pourcentages du dataset à sélectionner\n","    percentages = [0.02, 0.05,0.10, 0.20, 0.50, 0.70, 1.00]\n","    previousPercent = 0\n","\n","    dataset = np.concatenate((activeL_classical_list, activeL_non_classical_list))\n","\n","    for percent in percentages:\n","        print(f\"\"\"  for item in data:\n","      if any(np.array_equal(item, x) for x in activeL_classical_list):\n","          new_active_classical.append(item)\n","      else:\n","          new_active_non_classical.append(item)\n","\n","        #############################\n","        #         NEW ROUND         #\n","        #     percent = {percent}   #\n","        #############################\n","        \"\"\")\n","        lr, weight_decay, epochs,timeout = 1e-5, 5e-4, 1000, 30\n","        net = torch.load(modelToTrain)\n","        loss = torch.nn.CrossEntropyLoss()\n","        optimizer = torch.optim.Adam(net.parameters(),lr=lr, weight_decay=weight_decay)\n","\n","        # Active Learning\n","        print(f\"\"\"\n","        #############################\n","        #    Calculating Least      #\n","        #############################\n","        \"\"\")\n","        uncertainties = LeastConfidence(net, dataset)\n","        dataloader, dataset = LeastDataLoader(percent, previousPercent, uncertainties, dataset)\n","\n","        print(f\"\"\"\n","        #############################\n","        #       Begin Training      #\n","        #############################\n","        \"\"\")\n","\n","        net2 = train(net, dataloader, loss, optimizer, None, epochs, device, timeout=timeout)\n","        torch.save(net2, \"/content/drive/MyDrive/ColabNotebooks/models/LeastConfidence/least_confidence_\" + str(percent) + \"_percent.pt\")\n","\n","\n","\n","        # Faire des prédictions sur la liste d'évaluation\n","        model = net2\n","        model.eval()\n","        test_data=torch.Tensor(np.array(evaluation_list)).to(device)\n","        with torch.no_grad():\n","          preds = model(test_data)\n","        output=torch.argmax(preds, dim=1)\n","\n","        # Evaluation du modèle\n","        count = 0\n","        for i in range(0,len(output)):\n","            if output[i].item() == evaluation_list_dic[i][1]:\n","                count+=1\n","        print(f\"Least_Confidence {percent} % accuracy :{(count/len(output))*100}) + %\")\n","        print(output)\n","\n","        accuracy = (count/len(output))*100, epochs\n","\n","        # Ajouter les résultats à la liste\n","        results.append([percent, accuracy])\n","        previousPercent = percent\n","        # Utiliser le modèle précédent pour poursuivre l'entraînement\n","        modelToTrain = \"/content/drive/MyDrive/ColabNotebooks/models/LeastConfidence/least_confidence_\" + str(percent) + \"_percent.pt\"\n","\n","    # Créer un DataFrame à partir des résultats\n","    results_df = pd.DataFrame(results, columns=['Percentage', 'Accuracy'])\n","\n","    # Sauvegarder les résultats dans un fichier CSV\n","    results_csv_path = os.path.join(save_dir, 'results.csv')\n","    results_df.to_csv(results_csv_path, index=False)\n","\n","    return results_df\n"]},{"cell_type":"code","source":["leastConfidence_Train(\"/content/drive/MyDrive/ColabNotebooks/model_2_Clem_trainings_longEgale_Colab.pt\", evaluation_list)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"gZDpRn5eehl_","executionInfo":{"status":"ok","timestamp":1702995026951,"user_tz":-60,"elapsed":327761,"user":{"displayName":"Clemzi","userId":"17076433745448136774"}},"outputId":"ceb48052-bb09-4279-87fc-ca01baedce89"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["  for item in data:\n","      if any(np.array_equal(item, x) for x in activeL_classical_list):\n","          new_active_classical.append(item)\n","      else:\n","          new_active_non_classical.append(item)\n","\n","        #############################\n","        #         NEW ROUND         #\n","        #     percent = 0.02   #\n","        #############################\n","        \n","\n","        #############################\n","        #    Calculating Least      #\n","        #############################\n","        \n","337: 0.4978675842285156\n","124: 0.49786341190338135\n","372: 0.4978441596031189\n","467: 0.49784135818481445\n","339: 0.497836709022522\n","554: 0.49783509969711304\n","585: 0.4978329539299011\n","271: 0.4978228807449341\n","619: 0.4978196620941162\n","692: 0.497819185256958\n","[337, 124, 372, 467, 339, 554, 585, 271, 619, 692]\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-2-2cb1ec0aafdd>:39: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:261.)\n","  tensor_x = torch.Tensor(new_active_non_classical).to(device)\n"]},{"output_type":"stream","name":"stdout","text":["\n","        #############################\n","        #       Begin Training      #\n","        #############################\n","        \n","Training for 1000 epochs on cuda\n","Epoch 1/1000\n","Training loss: 0.87\n","Training accuracy: 0.00\n","\n","Epoch 2/1000\n","Training loss: 0.87\n","Training accuracy: 0.00\n","\n","Epoch 3/1000\n","Training loss: 0.87\n","Training accuracy: 0.00\n","\n","Epoch 4/1000\n","Training loss: 0.87\n","Training accuracy: 0.00\n","\n","Epoch 5/1000\n","Training loss: 0.87\n","Training accuracy: 0.00\n","\n","Epoch 6/1000\n","Training loss: 0.87\n","Training accuracy: 12.50\n","\n","Epoch 7/1000\n","Training loss: 0.87\n","Training accuracy: 12.50\n","\n","Epoch 8/1000\n","Training loss: 0.87\n","Training accuracy: 12.50\n","\n","Epoch 9/1000\n","Training loss: 0.87\n","Training accuracy: 12.50\n","\n","Epoch 10/1000\n","Training loss: 0.87\n","Training accuracy: 18.75\n","\n","Epoch 11/1000\n","Training loss: 0.87\n","Training accuracy: 31.25\n","\n","Epoch 12/1000\n","Training loss: 0.87\n","Training accuracy: 31.25\n","\n","Epoch 13/1000\n","Training loss: 0.87\n","Training accuracy: 31.25\n","\n","Epoch 14/1000\n","Training loss: 0.87\n","Training accuracy: 37.50\n","\n","Epoch 15/1000\n","Training loss: 0.87\n","Training accuracy: 43.75\n","\n","Epoch 16/1000\n","Training loss: 0.87\n","Training accuracy: 43.75\n","\n","Epoch 17/1000\n","Training loss: 0.87\n","Training accuracy: 43.75\n","\n","Epoch 18/1000\n","Training loss: 0.87\n","Training accuracy: 31.25\n","\n","Epoch 19/1000\n","Training loss: 0.87\n","Training accuracy: 43.75\n","\n","Epoch 20/1000\n","Training loss: 0.87\n","Training accuracy: 56.25\n","\n","Epoch 21/1000\n","Training loss: 0.87\n","Training accuracy: 56.25\n","\n","Epoch 22/1000\n","Training loss: 0.87\n","Training accuracy: 62.50\n","\n","Epoch 23/1000\n","Training loss: 0.87\n","Training accuracy: 62.50\n","\n","Epoch 24/1000\n","Training loss: 0.87\n","Training accuracy: 62.50\n","\n","Epoch 25/1000\n","Training loss: 0.87\n","Training accuracy: 56.25\n","\n","Epoch 26/1000\n","Training loss: 0.87\n","Training accuracy: 68.75\n","\n","Epoch 27/1000\n","Training loss: 0.87\n","Training accuracy: 68.75\n","\n","Epoch 28/1000\n","Training loss: 0.87\n","Training accuracy: 68.75\n","\n","Epoch 29/1000\n","Training loss: 0.87\n","Training accuracy: 68.75\n","\n","Epoch 30/1000\n","Training loss: 0.87\n","Training accuracy: 68.75\n","\n","Epoch 31/1000\n","Training loss: 0.87\n","Training accuracy: 68.75\n","\n","Epoch 32/1000\n","Training loss: 0.87\n","Training accuracy: 68.75\n","\n","Epoch 33/1000\n","Training loss: 0.87\n","Training accuracy: 68.75\n","\n","Epoch 34/1000\n","Training loss: 0.87\n","Training accuracy: 68.75\n","\n","Epoch 35/1000\n","Training loss: 0.87\n","Training accuracy: 68.75\n","\n","Epoch 36/1000\n","Training loss: 0.87\n","Training accuracy: 75.00\n","\n","Epoch 37/1000\n","Training loss: 0.87\n","Training accuracy: 75.00\n","\n","Epoch 38/1000\n","Training loss: 0.87\n","Training accuracy: 75.00\n","\n","Epoch 39/1000\n","Training loss: 0.87\n","Training accuracy: 81.25\n","\n","Epoch 40/1000\n","Training loss: 0.87\n","Training accuracy: 81.25\n","\n","Epoch 41/1000\n","Training loss: 0.87\n","Training accuracy: 75.00\n","\n","Epoch 42/1000\n","Training loss: 0.87\n","Training accuracy: 81.25\n","\n","Epoch 43/1000\n","Training loss: 0.87\n","Training accuracy: 81.25\n","\n","Epoch 44/1000\n","Training loss: 0.87\n","Training accuracy: 87.50\n","\n","Epoch 45/1000\n","Training loss: 0.87\n","Training accuracy: 87.50\n","\n","Epoch 46/1000\n","Training loss: 0.87\n","Training accuracy: 87.50\n","\n","Epoch 47/1000\n","Training loss: 0.87\n","Training accuracy: 87.50\n","\n","Epoch 48/1000\n","Training loss: 0.87\n","Training accuracy: 87.50\n","\n","Total training time: 30.5 seconds\n","Least_Confidence 0.02 % accuracy :13.5678391959799) + %\n","tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n","        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n","  for item in data:\n","      if any(np.array_equal(item, x) for x in activeL_classical_list):\n","          new_active_classical.append(item)\n","      else:\n","          new_active_non_classical.append(item)\n","\n","        #############################\n","        #         NEW ROUND         #\n","        #     percent = 0.05   #\n","        #############################\n","        \n","\n","        #############################\n","        #    Calculating Least      #\n","        #############################\n","        \n","729: 0.49854785203933716\n","283: 0.49847084283828735\n","440: 0.4984663724899292\n","479: 0.4984632730484009\n","538: 0.4984533190727234\n","716: 0.4984533190727234\n","444: 0.498452365398407\n","179: 0.4984501600265503\n","738: 0.49844640493392944\n","662: 0.49844467639923096\n","[729, 283, 440, 479, 538, 716, 444, 179, 738, 662]\n","\n","        #############################\n","        #       Begin Training      #\n","        #############################\n","        \n","Training for 1000 epochs on cuda\n","Epoch 1/1000\n","Training loss: 0.91\n","Training accuracy: 0.00\n","\n","Epoch 2/1000\n","Training loss: 0.91\n","Training accuracy: 4.35\n","\n","Epoch 3/1000\n","Training loss: 0.91\n","Training accuracy: 4.35\n","\n","Epoch 4/1000\n","Training loss: 0.91\n","Training accuracy: 13.04\n","\n","Epoch 5/1000\n","Training loss: 0.91\n","Training accuracy: 26.09\n","\n","Epoch 6/1000\n","Training loss: 0.90\n","Training accuracy: 21.74\n","\n","Epoch 7/1000\n","Training loss: 0.90\n","Training accuracy: 47.83\n","\n","Epoch 8/1000\n","Training loss: 0.90\n","Training accuracy: 47.83\n","\n","Epoch 9/1000\n","Training loss: 0.90\n","Training accuracy: 52.17\n","\n","Epoch 10/1000\n","Training loss: 0.90\n","Training accuracy: 65.22\n","\n","Epoch 11/1000\n","Training loss: 0.90\n","Training accuracy: 60.87\n","\n","Epoch 12/1000\n","Training loss: 0.90\n","Training accuracy: 82.61\n","\n","Epoch 13/1000\n","Training loss: 0.90\n","Training accuracy: 73.91\n","\n","Epoch 14/1000\n","Training loss: 0.90\n","Training accuracy: 86.96\n","\n","Epoch 15/1000\n","Training loss: 0.90\n","Training accuracy: 91.30\n","\n","Epoch 16/1000\n","Training loss: 0.90\n","Training accuracy: 91.30\n","\n","Epoch 17/1000\n","Training loss: 0.90\n","Training accuracy: 86.96\n","\n","Epoch 18/1000\n","Training loss: 0.90\n","Training accuracy: 86.96\n","\n","Epoch 19/1000\n","Training loss: 0.90\n","Training accuracy: 95.65\n","\n","Epoch 20/1000\n","Training loss: 0.90\n","Training accuracy: 100.00\n","\n","Epoch 21/1000\n","Training loss: 0.90\n","Training accuracy: 95.65\n","\n","Epoch 22/1000\n","Training loss: 0.90\n","Training accuracy: 100.00\n","\n","Epoch 23/1000\n","Training loss: 0.90\n","Training accuracy: 91.30\n","\n","Epoch 24/1000\n","Training loss: 0.90\n","Training accuracy: 100.00\n","\n","Epoch 25/1000\n","Training loss: 0.90\n","Training accuracy: 100.00\n","\n","Epoch 26/1000\n","Training loss: 0.90\n","Training accuracy: 100.00\n","\n","Epoch 27/1000\n","Training loss: 0.90\n","Training accuracy: 95.65\n","\n","Epoch 28/1000\n","Training loss: 0.90\n","Training accuracy: 100.00\n","\n","Epoch 29/1000\n","Training loss: 0.90\n","Training accuracy: 100.00\n","\n","Epoch 30/1000\n","Training loss: 0.90\n","Training accuracy: 100.00\n","\n","Epoch 31/1000\n","Training loss: 0.90\n","Training accuracy: 100.00\n","\n","Epoch 32/1000\n","Training loss: 0.90\n","Training accuracy: 100.00\n","\n","Epoch 33/1000\n","Training loss: 0.90\n","Training accuracy: 100.00\n","\n","Epoch 34/1000\n","Training loss: 0.90\n","Training accuracy: 100.00\n","\n","Epoch 35/1000\n","Training loss: 0.90\n","Training accuracy: 100.00\n","\n","Epoch 36/1000\n","Training loss: 0.90\n","Training accuracy: 100.00\n","\n","Epoch 37/1000\n","Training loss: 0.90\n","Training accuracy: 100.00\n","\n","Epoch 38/1000\n","Training loss: 0.90\n","Training accuracy: 100.00\n","\n","Total training time: 30.2 seconds\n","Least_Confidence 0.05 % accuracy :23.115577889447238) + %\n","tensor([0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,\n","        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n","        1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,\n","        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1,\n","        0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n","  for item in data:\n","      if any(np.array_equal(item, x) for x in activeL_classical_list):\n","          new_active_classical.append(item)\n","      else:\n","          new_active_non_classical.append(item)\n","\n","        #############################\n","        #         NEW ROUND         #\n","        #     percent = 0.1   #\n","        #############################\n","        \n","\n","        #############################\n","        #    Calculating Least      #\n","        #############################\n","        \n","208: 0.4999980330467224\n","737: 0.4999968409538269\n","539: 0.4999914765357971\n","662: 0.4999871850013733\n","438: 0.499977707862854\n","755: 0.49997609853744507\n","459: 0.4999721050262451\n","363: 0.49996763467788696\n","676: 0.4999579191207886\n","239: 0.49994897842407227\n","[208, 737, 539, 662, 438, 755, 459, 363, 676, 239]\n","\n","        #############################\n","        #       Begin Training      #\n","        #############################\n","        \n","Training for 1000 epochs on cuda\n","Epoch 1/1000\n","Training loss: 0.73\n","Training accuracy: 23.68\n","\n","Epoch 2/1000\n","Training loss: 0.73\n","Training accuracy: 50.00\n","\n","Epoch 3/1000\n","Training loss: 0.73\n","Training accuracy: 68.42\n","\n","Epoch 4/1000\n","Training loss: 0.73\n","Training accuracy: 78.95\n","\n","Epoch 5/1000\n","Training loss: 0.73\n","Training accuracy: 89.47\n","\n","Epoch 6/1000\n","Training loss: 0.73\n","Training accuracy: 94.74\n","\n","Epoch 7/1000\n","Training loss: 0.73\n","Training accuracy: 94.74\n","\n","Epoch 8/1000\n","Training loss: 0.73\n","Training accuracy: 97.37\n","\n","Epoch 9/1000\n","Training loss: 0.73\n","Training accuracy: 100.00\n","\n","Epoch 10/1000\n","Training loss: 0.73\n","Training accuracy: 100.00\n","\n","Epoch 11/1000\n","Training loss: 0.73\n","Training accuracy: 100.00\n","\n","Epoch 12/1000\n","Training loss: 0.73\n","Training accuracy: 100.00\n","\n","Epoch 13/1000\n","Training loss: 0.73\n","Training accuracy: 100.00\n","\n","Epoch 14/1000\n","Training loss: 0.73\n","Training accuracy: 100.00\n","\n","Epoch 15/1000\n","Training loss: 0.73\n","Training accuracy: 100.00\n","\n","Epoch 16/1000\n","Training loss: 0.73\n","Training accuracy: 100.00\n","\n","Epoch 17/1000\n","Training loss: 0.73\n","Training accuracy: 100.00\n","\n","Epoch 18/1000\n","Training loss: 0.73\n","Training accuracy: 100.00\n","\n","Epoch 19/1000\n","Training loss: 0.73\n","Training accuracy: 100.00\n","\n","Epoch 20/1000\n","Training loss: 0.73\n","Training accuracy: 100.00\n","\n","Epoch 21/1000\n","Training loss: 0.73\n","Training accuracy: 100.00\n","\n","Epoch 22/1000\n","Training loss: 0.73\n","Training accuracy: 100.00\n","\n","Epoch 23/1000\n","Training loss: 0.73\n","Training accuracy: 100.00\n","\n","Epoch 24/1000\n","Training loss: 0.73\n","Training accuracy: 100.00\n","\n","Epoch 25/1000\n","Training loss: 0.73\n","Training accuracy: 100.00\n","\n","Epoch 26/1000\n","Training loss: 0.73\n","Training accuracy: 100.00\n","\n","Epoch 27/1000\n","Training loss: 0.73\n","Training accuracy: 100.00\n","\n","Epoch 28/1000\n","Training loss: 0.73\n","Training accuracy: 100.00\n","\n","Epoch 29/1000\n","Training loss: 0.73\n","Training accuracy: 100.00\n","\n","Epoch 30/1000\n","Training loss: 0.73\n","Training accuracy: 100.00\n","\n","Epoch 31/1000\n","Training loss: 0.73\n","Training accuracy: 100.00\n","\n","Epoch 32/1000\n","Training loss: 0.73\n","Training accuracy: 100.00\n","\n","Total training time: 30.0 seconds\n","Least_Confidence 0.1 % accuracy :69.84924623115577) + %\n","tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,\n","        0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1,\n","        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,\n","        0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,\n","        0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1,\n","        1, 1, 0, 1, 1, 0, 1], device='cuda:0')\n","  for item in data:\n","      if any(np.array_equal(item, x) for x in activeL_classical_list):\n","          new_active_classical.append(item)\n","      else:\n","          new_active_non_classical.append(item)\n","\n","        #############################\n","        #         NEW ROUND         #\n","        #     percent = 0.2   #\n","        #############################\n","        \n","\n","        #############################\n","        #    Calculating Least      #\n","        #############################\n","        \n","463: 0.4999995827674866\n","542: 0.4999995827674866\n","99: 0.49999070167541504\n","48: 0.49999064207077026\n","717: 0.49997931718826294\n","332: 0.49996793270111084\n","611: 0.4999641180038452\n","519: 0.49996232986450195\n","700: 0.4999493360519409\n","464: 0.4999461770057678\n","[463, 542, 99, 48, 717, 332, 611, 519, 700, 464]\n","\n","        #############################\n","        #       Begin Training      #\n","        #############################\n","        \n","Training for 1000 epochs on cuda\n","Epoch 1/1000\n","Training loss: 0.77\n","Training accuracy: 50.00\n","\n","Epoch 2/1000\n","Training loss: 0.77\n","Training accuracy: 80.56\n","\n","Epoch 3/1000\n","Training loss: 0.77\n","Training accuracy: 84.72\n","\n","Epoch 4/1000\n","Training loss: 0.77\n","Training accuracy: 93.06\n","\n","Epoch 5/1000\n","Training loss: 0.77\n","Training accuracy: 95.83\n","\n","Epoch 6/1000\n","Training loss: 0.77\n","Training accuracy: 93.06\n","\n","Epoch 7/1000\n","Training loss: 0.77\n","Training accuracy: 95.83\n","\n","Epoch 8/1000\n","Training loss: 0.77\n","Training accuracy: 98.61\n","\n","Epoch 9/1000\n","Training loss: 0.77\n","Training accuracy: 97.22\n","\n","Epoch 10/1000\n","Training loss: 0.77\n","Training accuracy: 95.83\n","\n","Epoch 11/1000\n","Training loss: 0.77\n","Training accuracy: 97.22\n","\n","Epoch 12/1000\n","Training loss: 0.77\n","Training accuracy: 98.61\n","\n","Epoch 13/1000\n","Training loss: 0.77\n","Training accuracy: 100.00\n","\n","Epoch 14/1000\n","Training loss: 0.77\n","Training accuracy: 100.00\n","\n","Epoch 15/1000\n","Training loss: 0.77\n","Training accuracy: 100.00\n","\n","Epoch 16/1000\n","Training loss: 0.77\n","Training accuracy: 98.61\n","\n","Epoch 17/1000\n","Training loss: 0.77\n","Training accuracy: 100.00\n","\n","Epoch 18/1000\n","Training loss: 0.77\n","Training accuracy: 98.61\n","\n","Epoch 19/1000\n","Training loss: 0.77\n","Training accuracy: 100.00\n","\n","Epoch 20/1000\n","Training loss: 0.77\n","Training accuracy: 100.00\n","\n","Epoch 21/1000\n","Training loss: 0.77\n","Training accuracy: 100.00\n","\n","Epoch 22/1000\n","Training loss: 0.77\n","Training accuracy: 98.61\n","\n","Total training time: 30.3 seconds\n","Least_Confidence 0.2 % accuracy :73.86934673366834) + %\n","tensor([0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,\n","        1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0,\n","        1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1,\n","        1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0,\n","        1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1,\n","        1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1,\n","        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1,\n","        1, 0, 1, 1, 0, 1, 0], device='cuda:0')\n","  for item in data:\n","      if any(np.array_equal(item, x) for x in activeL_classical_list):\n","          new_active_classical.append(item)\n","      else:\n","          new_active_non_classical.append(item)\n","\n","        #############################\n","        #         NEW ROUND         #\n","        #     percent = 0.5   #\n","        #############################\n","        \n","\n","        #############################\n","        #    Calculating Least      #\n","        #############################\n","        \n","504: 0.49999457597732544\n","341: 0.4999871253967285\n","348: 0.4999837875366211\n","122: 0.4999728202819824\n","57: 0.4999704360961914\n","438: 0.49996834993362427\n","66: 0.49995648860931396\n","163: 0.4999493956565857\n","538: 0.49994349479675293\n","314: 0.49993419647216797\n","[504, 341, 348, 122, 57, 438, 66, 163, 538, 314]\n","\n","        #############################\n","        #       Begin Training      #\n","        #############################\n","        \n","Training for 1000 epochs on cuda\n","Epoch 1/1000\n","Training loss: 0.71\n","Training accuracy: 90.26\n","\n","Epoch 2/1000\n","Training loss: 0.71\n","Training accuracy: 94.36\n","\n","Epoch 3/1000\n","Training loss: 0.71\n","Training accuracy: 94.36\n","\n","Epoch 4/1000\n","Training loss: 0.71\n","Training accuracy: 95.38\n","\n","Epoch 5/1000\n","Training loss: 0.71\n","Training accuracy: 95.90\n","\n","Epoch 6/1000\n","Training loss: 0.71\n","Training accuracy: 95.38\n","\n","Epoch 7/1000\n","Training loss: 0.71\n","Training accuracy: 96.41\n","\n","Epoch 8/1000\n","Training loss: 0.71\n","Training accuracy: 96.41\n","\n","Epoch 9/1000\n","Training loss: 0.71\n","Training accuracy: 95.90\n","\n","Epoch 10/1000\n","Training loss: 0.70\n","Training accuracy: 96.92\n","\n","Total training time: 31.7 seconds\n","Least_Confidence 0.5 % accuracy :89.9497487437186) + %\n","tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n","  for item in data:\n","      if any(np.array_equal(item, x) for x in activeL_classical_list):\n","          new_active_classical.append(item)\n","      else:\n","          new_active_non_classical.append(item)\n","\n","        #############################\n","        #         NEW ROUND         #\n","        #     percent = 0.7   #\n","        #############################\n","        \n","\n","        #############################\n","        #    Calculating Least      #\n","        #############################\n","        \n","89: 0.4978429675102234\n","11: 0.4977594017982483\n","55: 0.49690431356430054\n","33: 0.4964447021484375\n","2: 0.49626481533050537\n","28: 0.49618715047836304\n","36: 0.4957258701324463\n","233: 0.49545717239379883\n","16: 0.49545520544052124\n","147: 0.49505603313446045\n","[89, 11, 55, 33, 2, 28, 36, 233, 16, 147]\n","\n","        #############################\n","        #       Begin Training      #\n","        #############################\n","        \n","Training for 1000 epochs on cuda\n","Epoch 1/1000\n","Training loss: 0.76\n","Training accuracy: 78.02\n","\n","Epoch 2/1000\n","Training loss: 0.76\n","Training accuracy: 78.02\n","\n","Epoch 3/1000\n","Training loss: 0.76\n","Training accuracy: 79.12\n","\n","Epoch 4/1000\n","Training loss: 0.76\n","Training accuracy: 79.12\n","\n","Epoch 5/1000\n","Training loss: 0.76\n","Training accuracy: 86.81\n","\n","Epoch 6/1000\n","Training loss: 0.76\n","Training accuracy: 87.91\n","\n","Epoch 7/1000\n","Training loss: 0.76\n","Training accuracy: 93.41\n","\n","Epoch 8/1000\n","Training loss: 0.76\n","Training accuracy: 95.60\n","\n","Epoch 9/1000\n","Training loss: 0.76\n","Training accuracy: 95.60\n","\n","Epoch 10/1000\n","Training loss: 0.75\n","Training accuracy: 97.80\n","\n","Epoch 11/1000\n","Training loss: 0.75\n","Training accuracy: 95.60\n","\n","Epoch 12/1000\n","Training loss: 0.76\n","Training accuracy: 95.60\n","\n","Epoch 13/1000\n","Training loss: 0.75\n","Training accuracy: 97.80\n","\n","Epoch 14/1000\n","Training loss: 0.75\n","Training accuracy: 97.80\n","\n","Epoch 15/1000\n","Training loss: 0.75\n","Training accuracy: 97.80\n","\n","Epoch 16/1000\n","Training loss: 0.75\n","Training accuracy: 97.80\n","\n","Epoch 17/1000\n","Training loss: 0.75\n","Training accuracy: 98.90\n","\n","Epoch 18/1000\n","Training loss: 0.75\n","Training accuracy: 98.90\n","\n","Epoch 19/1000\n","Training loss: 0.75\n","Training accuracy: 98.90\n","\n","Total training time: 30.5 seconds\n","Least_Confidence 0.7 % accuracy :91.95979899497488) + %\n","tensor([1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,\n","        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,\n","        1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n","        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 0], device='cuda:0')\n","  for item in data:\n","      if any(np.array_equal(item, x) for x in activeL_classical_list):\n","          new_active_classical.append(item)\n","      else:\n","          new_active_non_classical.append(item)\n","\n","        #############################\n","        #         NEW ROUND         #\n","        #     percent = 1.0   #\n","        #############################\n","        \n","\n","        #############################\n","        #    Calculating Least      #\n","        #############################\n","        \n","19: 0.4968401789665222\n","168: 0.496288537979126\n","72: 0.4962831139564514\n","95: 0.4962146282196045\n","4: 0.49592316150665283\n","13: 0.4958783984184265\n","2: 0.4955386519432068\n","3: 0.4954819679260254\n","73: 0.4954116940498352\n","31: 0.4951159358024597\n","[19, 168, 72, 95, 4, 13, 2, 3, 73, 31]\n","\n","        #############################\n","        #       Begin Training      #\n","        #############################\n","        \n","Training for 1000 epochs on cuda\n","Epoch 1/1000\n","Training loss: 0.69\n","Training accuracy: 87.16\n","\n","Epoch 2/1000\n","Training loss: 0.69\n","Training accuracy: 86.24\n","\n","Epoch 3/1000\n","Training loss: 0.69\n","Training accuracy: 91.74\n","\n","Epoch 4/1000\n","Training loss: 0.69\n","Training accuracy: 94.50\n","\n","Epoch 5/1000\n","Training loss: 0.69\n","Training accuracy: 96.33\n","\n","Epoch 6/1000\n","Training loss: 0.69\n","Training accuracy: 98.17\n","\n","Epoch 7/1000\n","Training loss: 0.69\n","Training accuracy: 97.25\n","\n","Epoch 8/1000\n","Training loss: 0.69\n","Training accuracy: 98.17\n","\n","Epoch 9/1000\n","Training loss: 0.69\n","Training accuracy: 98.17\n","\n","Epoch 10/1000\n","Training loss: 0.69\n","Training accuracy: 98.17\n","\n","Epoch 11/1000\n","Training loss: 0.69\n","Training accuracy: 98.17\n","\n","Epoch 12/1000\n","Training loss: 0.69\n","Training accuracy: 98.17\n","\n","Epoch 13/1000\n","Training loss: 0.69\n","Training accuracy: 98.17\n","\n","Epoch 14/1000\n","Training loss: 0.69\n","Training accuracy: 98.17\n","\n","Epoch 15/1000\n","Training loss: 0.69\n","Training accuracy: 98.17\n","\n","Epoch 16/1000\n","Training loss: 0.69\n","Training accuracy: 98.17\n","\n","Total training time: 30.1 seconds\n","Least_Confidence 1.0 % accuracy :94.47236180904522) + %\n","tensor([1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 0], device='cuda:0')\n"]},{"output_type":"execute_result","data":{"text/plain":["   Percentage                    Accuracy\n","0        0.02    (13.5678391959799, 1000)\n","1        0.05  (23.115577889447238, 1000)\n","2        0.10   (69.84924623115577, 1000)\n","3        0.20   (73.86934673366834, 1000)\n","4        0.50    (89.9497487437186, 1000)\n","5        0.70   (91.95979899497488, 1000)\n","6        1.00   (94.47236180904522, 1000)"],"text/html":["\n","  <div id=\"df-eec78675-5c6e-4bcb-ab2d-edb7ec9be6f3\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Percentage</th>\n","      <th>Accuracy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.02</td>\n","      <td>(13.5678391959799, 1000)</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.05</td>\n","      <td>(23.115577889447238, 1000)</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.10</td>\n","      <td>(69.84924623115577, 1000)</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.20</td>\n","      <td>(73.86934673366834, 1000)</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.50</td>\n","      <td>(89.9497487437186, 1000)</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>0.70</td>\n","      <td>(91.95979899497488, 1000)</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>1.00</td>\n","      <td>(94.47236180904522, 1000)</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-eec78675-5c6e-4bcb-ab2d-edb7ec9be6f3')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-eec78675-5c6e-4bcb-ab2d-edb7ec9be6f3 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-eec78675-5c6e-4bcb-ab2d-edb7ec9be6f3');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-c1dc1a63-0da9-45bc-8fd9-3af86e272917\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-c1dc1a63-0da9-45bc-8fd9-3af86e272917')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-c1dc1a63-0da9-45bc-8fd9-3af86e272917 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":5}]}]}