{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyN6uSnulNOLiYNWVxlMcoUM"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","from urllib.request import urlopen\n","from PIL import Image\n","import torch\n","from transformers import AutoFeatureExtractor, ResNetForImageClassification, ResNetModel\n","import os\n","from PIL import Image\n","from torch.utils.data import TensorDataset, DataLoader\n","import time\n","import numpy as np\n","device = 'cuda'\n","from transformers import ResNetModel\n","import torch\n","\n","def train(net, train_dataloader, criterion, optimizer, scheduler=None, epochs=100, device=device, checkpoint_epochs=2, timeout=45):\n","    start = time.time()\n","    print(f'Training for {epochs} epochs on {device}')\n","\n","    for epoch in range(1,epochs+1):\n","        print(f\"Epoch {epoch}/{epochs}\")\n","\n","        net.train()  # put network in train mode for Dropout and Batch Normalization\n","        train_loss = torch.tensor(0., device=device)  # loss and accuracy tensors are on the GPU to avoid data transfers\n","        train_accuracy = torch.tensor(0., device=device)\n","        for X, y in train_dataloader:\n","            X = X.to(device)\n","            y = y.type(torch.LongTensor).to(device)\n","            preds = net(X)\n","            loss = criterion(preds, y)\n","\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","            with torch.no_grad():\n","                train_loss += loss * train_dataloader.batch_size\n","                train_accuracy += (torch.argmax(preds, dim=1) == y).sum()\n","\n","        if scheduler is not None:\n","            scheduler.step()\n","\n","        print(f'Training loss: {train_loss/len(train_dataloader.dataset):.2f}')\n","        print(f'Training accuracy: {100*train_accuracy/len(train_dataloader.dataset):.2f}')\n","\n","\n","        if epoch%checkpoint_epochs==0:\n","            torch.save({\n","                'epoch': epoch,\n","                'state_dict': net.state_dict(),\n","                'optimizer': optimizer.state_dict(),\n","            }, './checkpoint.pth.tar')\n","\n","        print()\n","        if((time.time() - start) >= timeout):\n","          break\n","\n","    end = time.time()\n","    print(f'Total training time: {end-start:.1f} seconds')\n","    return net\n","\n","# model definition\n","class Classifier_model(torch.nn.Module):\n","    # define model elements\n","    def __init__(self):\n","        super(Classifier_model, self).__init__()\n","        self.device = device\n","        self.image_processor = AutoFeatureExtractor.from_pretrained(\"microsoft/resnet-18\",device=self.device)\n","        self.pre_trained_model = ResNetModel.from_pretrained(\"microsoft/resnet-18\")\n","        resnet18_output_size=25088\n","        self.fc = torch.nn.Linear(resnet18_output_size, 10)\n","        self.activation = torch.nn.ReLU()\n","\n","    # forward propagate input\n","    def forward(self, X):\n","        X = self.image_processor(X, return_tensors=\"pt\").to(self.device)\n","        # print(X.pixel_value.is_cuda)\n","        X = self.pre_trained_model(**X).last_hidden_state.flatten(start_dim=1)\n","        X = self.activation(X)\n","        X = self.fc(X)\n","\n","        return X.softmax(dim=1)\n","\n","    def features_extractor(self, X):\n","        X = self.image_processor(X, return_tensors=\"pt\").to(self.device)\n","        # print(X.pixel_value.is_cuda)\n","        X = self.pre_trained_model(**X).last_hidden_state.flatten(start_dim=1)\n","\n","        return X.softmax(dim=1)\n","\n","# model definition\n","class Classifier_model_2(torch.nn.Module):\n","    # define model elements\n","    def __init__(self,f_model):\n","        super(Classifier_model_2, self).__init__()\n","        self.device = device\n","        resnet18_output_size=25088\n","        self.f_model = f_model\n","        self.fc = torch.nn.Linear(resnet18_output_size, 2)\n","        self.activation = torch.nn.ReLU()\n","\n","    # forward propagate input\n","    def forward(self, X):\n","        #X = self.image_processor(X, return_tensors=\"pt\").to(self.device)\n","        # print(X.pixel_value.is_cuda)\n","        X = self.f_model.features_extractor(X)\n","        X = self.activation(X)\n","        X = self.fc(X)\n","\n","        return X.softmax(dim=1)\n","\n","\n","# Chemin du répertoire racine\n","import random\n","root_dir = '/content/drive/MyDrive/ColabNotebooks/mel'\n","\n","# Dictionnaire pour stocker les images par sous-dossier\n","image_dict = {}\n","\n","# Parcourir tous les sous-dossiers\n","for root, dirs, files in os.walk(root_dir):\n","    for file in files:\n","        # Construire le chemin complet du fichier\n","        file_path = os.path.join(root, file)\n","\n","        # Vérifier si le fichier est une image en fonction de l'extension (par exemple, .png)\n","        if file_path.lower().endswith(('.png', '.jpg', '.jpeg', '.gif')):\n","            # Ouvrir l'image avec Pillow\n","            image = Image.open(file_path)\n","\n","            # Convertir l'image en format JPG (si elle n'est pas déjà en JPG)\n","            if image.format != \"JPEG\":\n","                image = image.convert(\"RGB\")\n","\n","            # Obtenez le nom du sous-dossier parent\n","            parent_dir = os.path.basename(os.path.dirname(file_path))\n","\n","            # Vérifiez si le sous-dossier existe dans le dictionnaire, sinon créez-le\n","            if parent_dir not in image_dict:\n","                image_dict[parent_dir] = []\n","\n","            # Ajouter l'image à la liste du sous-dossier correspondant\n","            image = image.resize((336,219))\n","            image_dict[parent_dir].append(np.array(image))\n","\n","classical_list_MG = image_dict['classic']\n","non_classical_list_MG = image_dict['non_classic']\n","\n","percent_classical_MG = int(0.2*len(classical_list_MG))\n","percent_non_classical_MG = int(0.2*len(non_classical_list_MG))\n","\n","\n","evaluation_list_dic = []\n","for sample in classical_list_MG[:percent_classical_MG]:\n","    buffer = []\n","    buffer.append(sample)\n","    buffer.append(0)\n","    evaluation_list_dic.append(buffer)\n","\n","for sample in non_classical_list_MG[:percent_non_classical_MG]:\n","    buffer = []\n","    buffer.append(sample)\n","    buffer.append(1)\n","    evaluation_list_dic.append(buffer)\n","\n","random.shuffle(evaluation_list_dic)\n","evaluation_list = []\n","for sample in evaluation_list_dic:\n","    evaluation_list.append(sample[0])\n","\n","activeL_classical_list = classical_list_MG[percent_classical_MG:]\n","activeL_non_classical_list = non_classical_list_MG[percent_non_classical_MG:]\n"],"metadata":{"id":"09mt6poN8zpx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1702987268242,"user_tz":-60,"elapsed":47471,"user":{"displayName":"Clemzi","userId":"17076433745448136774"}},"outputId":"003a743a-0963-4b8b-fb73-62331c0c1052"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["def RatioDataLoader(percent, previousPercent, uncertainties, dataset):\n","  \"\"\"\n","  This function is used after having determined the Ratio of Confidence of the base dataset.\n","  Keywords:\n","    percent         : The percentage of the base dataset to take account of\n","    previousPercent : The previous percentage that was used the last time this function was called\n","    uncertainties   : A dictionnary containing the uncertainties associated with each sample of dataset\n","    dataset         : The updated dataset containing only unseen samples\n","  Returns:\n","    my_dataloader   : pytorch compatible data corresponding to the dataset used for the model training\n","    dataset         : The new dataset containing unseen samples\n","  \"\"\"\n","\n","  # How many samples do I have to care about this time ?\n","  difference_in_percent = percent - previousPercent\n","  lastPoint_uncertainties = int(difference_in_percent * len(uncertainties))\n","\n","  # Taking only that many samples into consideration\n","  samples_to_take = list(uncertainties.keys())[:lastPoint_uncertainties]\n","  print(samples_to_take[:10])\n","  # the indices inside the dataset of the most uncertained samples\n","  data = []\n","  for i in range(len(samples_to_take)):\n","    data.append(dataset[samples_to_take[i]])\n","\n","  dataset = np.delete(dataset, samples_to_take, axis=0)\n","\n","  # Now, we need to recreate two tensors knowing if each samples contained\n","  # inside the dataset is classical or not.\n","  new_active_classical = []\n","  new_active_non_classical = []\n","  for item in data:\n","      if any(np.array_equal(item, x) for x in activeL_classical_list):\n","          new_active_classical.append(item)\n","      else:\n","          new_active_non_classical.append(item)\n","\n","  if (len(new_active_classical) == 0):\n","    tensor_x = torch.Tensor(new_active_non_classical).to(device)\n","    tensor_y = torch.Tensor(np.full(len(new_active_non_classical),  1)).to(device)\n","\n","\n","  else:\n","    tensor_x = torch.Tensor(np.concatenate((new_active_classical, new_active_non_classical), axis = 0)).to(device)\n","    tensor_y = torch.Tensor(np.concatenate((np.full(len(new_active_classical), 0),\n","                                        np.full(len(new_active_non_classical),  1)), axis=0)).to(device)\n","\n","\n","  my_dataset = TensorDataset(tensor_x,tensor_y) # create your datset\n","  my_dataloader = DataLoader(my_dataset,batch_size=10, shuffle=True)\n","  return my_dataloader, dataset"],"metadata":{"id":"PXpyRHTiD4pX","executionInfo":{"status":"ok","timestamp":1702987268244,"user_tz":-60,"elapsed":18,"user":{"displayName":"Clemzi","userId":"17076433745448136774"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["def RatioOfConfidence(model, test_data):\n","  \"\"\"\n","  Keywords:\n","    model : The model that gives predictions\n","    data  : The dataset to base the predictions on\n","    n     : the number of best candidates to select\n","\n","  Returns:\n","    A sorted dictionnary containing the uncertainty score associated with a sample\n","  \"\"\"\n","  uncertainty_dict = {}\n","  for i in range(len(test_data)):\n","    preds = model(test_data[i])\n","    preds_sorted = np.sort(preds[0].cpu().detach().numpy())\n","    uncertainty_dict[i] = (preds_sorted[0] / preds_sorted[1])\n","  res = dict(sorted(uncertainty_dict.items(),\n","                    key = lambda x: x[1], reverse = True))\n","  for key in list(res.keys())[:10]:\n","      print(f\"{key}: {res[key]}\")\n","  return res"],"metadata":{"id":"rNbQ6GVnD-14","executionInfo":{"status":"ok","timestamp":1702987268245,"user_tz":-60,"elapsed":17,"user":{"displayName":"Clemzi","userId":"17076433745448136774"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","execution_count":4,"metadata":{"id":"St5iHgGYfZCn","executionInfo":{"status":"ok","timestamp":1702987269561,"user_tz":-60,"elapsed":1333,"user":{"displayName":"Clemzi","userId":"17076433745448136774"}}},"outputs":[],"source":["import os\n","import random\n","import numpy as np\n","import pandas as pd\n","import torch\n","from torch.utils.data import DataLoader, TensorDataset\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","\n","def ratioOfConfidence_Train(modelToTrain, evaluation_list,save_dir='/content/drive/MyDrive/ColabNotebooks/models/RatioOfConfidence'):\n","\n","    # Créer un répertoire s'il n'existe pas pour y stocker les modèles\n","    if not os.path.exists(save_dir):\n","        os.makedirs(save_dir)\n","\n","    results = []\n","\n","    # Pourcentages du dataset à sélectionner\n","    percentages = [0.02, 0.05,0.10, 0.20, 0.50, 0.70, 1.00]\n","    previousPercent = 0\n","\n","    dataset = np.concatenate((activeL_classical_list, activeL_non_classical_list))\n","\n","    for percent in percentages:\n","        print(f\"\"\"  for item in data:\n","      if any(np.array_equal(item, x) for x in activeL_classical_list):\n","          new_active_classical.append(item)\n","      else:\n","          new_active_non_classical.append(item)\n","\n","        #############################\n","        #         NEW ROUND         #\n","        #     percent = {percent}   #\n","        #############################\n","        \"\"\")\n","        lr, weight_decay, epochs,timeout = 1e-5, 5e-4, 1000, 30\n","        net = torch.load(modelToTrain)\n","        loss = torch.nn.CrossEntropyLoss()\n","        optimizer = torch.optim.Adam(net.parameters(),lr=lr, weight_decay=weight_decay)\n","\n","        # Active Learning\n","        print(f\"\"\"\n","        #############################\n","        #    Calculating Ratio     #\n","        #############################\n","        \"\"\")\n","        uncertainties = RatioOfConfidence(net, dataset)\n","        dataloader, dataset = RatioDataLoader(percent, previousPercent, uncertainties, dataset)\n","\n","        print(f\"\"\"\n","        #############################\n","        #       Begin Training      #\n","        #############################\n","        \"\"\")\n","\n","        net2 = train(net, dataloader, loss, optimizer, None, epochs, device, timeout=timeout)\n","        torch.save(net2, \"/content/drive/MyDrive/ColabNotebooks/models/RatioOfConfidence/ratio_confidence_\" + str(percent) + \"_percent.pt\")\n","\n","\n","\n","        # Faire des prédictions sur la liste d'évaluation\n","        model = net2\n","        model.eval()\n","        test_data=torch.Tensor(np.array(evaluation_list)).to(device)\n","        with torch.no_grad():\n","          preds = model(test_data)\n","        output=torch.argmax(preds, dim=1)\n","\n","        # Evaluation du modèle\n","        count = 0\n","        for i in range(0,len(output)):\n","            if output[i].item() == evaluation_list_dic[i][1]:\n","                count+=1\n","        print(f\"Ratio_Confidence {percent} % accuracy :{(count/len(output))*100}) + %\")\n","        print(output)\n","\n","        accuracy = (count/len(output))*100, epochs\n","\n","        # Ajouter les résultats à la liste\n","        results.append([percent, accuracy])\n","        previousPercent = percent\n","        # Utiliser le modèle précédent pour poursuivre l'entraînement\n","        modelToTrain = \"/content/drive/MyDrive/ColabNotebooks/models/RatioOfConfidence/ratio_confidence_\" + str(percent) + \"_percent.pt\"\n","\n","    # Créer un DataFrame à partir des résultats\n","    results_df = pd.DataFrame(results, columns=['Percentage', 'Accuracy'])\n","\n","    # Sauvegarder les résultats dans un fichier CSV\n","    results_csv_path = os.path.join(save_dir, 'results.csv')\n","    results_df.to_csv(results_csv_path, index=False)\n","\n","    return results_df\n"]},{"cell_type":"code","source":["ratioOfConfidence_Train(\"/content/drive/MyDrive/ColabNotebooks/model_2_Clem_trainings_longEgale_Colab.pt\", evaluation_list)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"gZDpRn5eehl_","executionInfo":{"status":"ok","timestamp":1702987596065,"user_tz":-60,"elapsed":326516,"user":{"displayName":"Clemzi","userId":"17076433745448136774"}},"outputId":"75eeb023-59ea-4f1a-fb54-e011f8c3b05e"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["  for item in data:\n","      if any(np.array_equal(item, x) for x in activeL_classical_list):\n","          new_active_classical.append(item)\n","      else:\n","          new_active_non_classical.append(item)\n","\n","        #############################\n","        #         NEW ROUND         #\n","        #     percent = 0.02   #\n","        #############################\n","        \n","\n","        #############################\n","        #    Calculating Ratio     #\n","        #############################\n","        \n","337: 0.9915065765380859\n","124: 0.9914900064468384\n","372: 0.9914135336875916\n","467: 0.9914026260375977\n","339: 0.9913840293884277\n","554: 0.9913778305053711\n","585: 0.9913693070411682\n","271: 0.9913291335105896\n","619: 0.991316556930542\n","692: 0.9913145899772644\n","[337, 124, 372, 467, 339, 554, 585, 271, 619, 692]\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-2-79a53560f93b>:39: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:261.)\n","  tensor_x = torch.Tensor(new_active_non_classical).to(device)\n"]},{"output_type":"stream","name":"stdout","text":["\n","        #############################\n","        #       Begin Training      #\n","        #############################\n","        \n","Training for 1000 epochs on cuda\n","Epoch 1/1000\n","Training loss: 0.87\n","Training accuracy: 0.00\n","\n","Epoch 2/1000\n","Training loss: 0.87\n","Training accuracy: 0.00\n","\n","Epoch 3/1000\n","Training loss: 0.87\n","Training accuracy: 0.00\n","\n","Epoch 4/1000\n","Training loss: 0.87\n","Training accuracy: 0.00\n","\n","Epoch 5/1000\n","Training loss: 0.87\n","Training accuracy: 6.25\n","\n","Epoch 6/1000\n","Training loss: 0.87\n","Training accuracy: 6.25\n","\n","Epoch 7/1000\n","Training loss: 0.87\n","Training accuracy: 0.00\n","\n","Epoch 8/1000\n","Training loss: 0.87\n","Training accuracy: 12.50\n","\n","Epoch 9/1000\n","Training loss: 0.87\n","Training accuracy: 25.00\n","\n","Epoch 10/1000\n","Training loss: 0.87\n","Training accuracy: 31.25\n","\n","Epoch 11/1000\n","Training loss: 0.87\n","Training accuracy: 25.00\n","\n","Epoch 12/1000\n","Training loss: 0.87\n","Training accuracy: 37.50\n","\n","Epoch 13/1000\n","Training loss: 0.87\n","Training accuracy: 31.25\n","\n","Epoch 14/1000\n","Training loss: 0.87\n","Training accuracy: 25.00\n","\n","Epoch 15/1000\n","Training loss: 0.87\n","Training accuracy: 43.75\n","\n","Epoch 16/1000\n","Training loss: 0.87\n","Training accuracy: 43.75\n","\n","Epoch 17/1000\n","Training loss: 0.87\n","Training accuracy: 43.75\n","\n","Epoch 18/1000\n","Training loss: 0.87\n","Training accuracy: 43.75\n","\n","Epoch 19/1000\n","Training loss: 0.87\n","Training accuracy: 50.00\n","\n","Epoch 20/1000\n","Training loss: 0.87\n","Training accuracy: 62.50\n","\n","Epoch 21/1000\n","Training loss: 0.87\n","Training accuracy: 56.25\n","\n","Epoch 22/1000\n","Training loss: 0.87\n","Training accuracy: 56.25\n","\n","Epoch 23/1000\n","Training loss: 0.87\n","Training accuracy: 43.75\n","\n","Epoch 24/1000\n","Training loss: 0.87\n","Training accuracy: 75.00\n","\n","Epoch 25/1000\n","Training loss: 0.87\n","Training accuracy: 62.50\n","\n","Epoch 26/1000\n","Training loss: 0.87\n","Training accuracy: 81.25\n","\n","Epoch 27/1000\n","Training loss: 0.87\n","Training accuracy: 81.25\n","\n","Epoch 28/1000\n","Training loss: 0.87\n","Training accuracy: 87.50\n","\n","Epoch 29/1000\n","Training loss: 0.87\n","Training accuracy: 81.25\n","\n","Epoch 30/1000\n","Training loss: 0.87\n","Training accuracy: 68.75\n","\n","Epoch 31/1000\n","Training loss: 0.87\n","Training accuracy: 87.50\n","\n","Epoch 32/1000\n","Training loss: 0.87\n","Training accuracy: 87.50\n","\n","Epoch 33/1000\n","Training loss: 0.87\n","Training accuracy: 87.50\n","\n","Epoch 34/1000\n","Training loss: 0.87\n","Training accuracy: 87.50\n","\n","Epoch 35/1000\n","Training loss: 0.87\n","Training accuracy: 87.50\n","\n","Epoch 36/1000\n","Training loss: 0.87\n","Training accuracy: 87.50\n","\n","Epoch 37/1000\n","Training loss: 0.87\n","Training accuracy: 87.50\n","\n","Epoch 38/1000\n","Training loss: 0.87\n","Training accuracy: 87.50\n","\n","Epoch 39/1000\n","Training loss: 0.87\n","Training accuracy: 87.50\n","\n","Epoch 40/1000\n","Training loss: 0.87\n","Training accuracy: 87.50\n","\n","Epoch 41/1000\n","Training loss: 0.87\n","Training accuracy: 87.50\n","\n","Epoch 42/1000\n","Training loss: 0.87\n","Training accuracy: 87.50\n","\n","Epoch 43/1000\n","Training loss: 0.87\n","Training accuracy: 87.50\n","\n","Epoch 44/1000\n","Training loss: 0.87\n","Training accuracy: 87.50\n","\n","Total training time: 30.1 seconds\n","Ratio_Confidence 0.02 % accuracy :14.07035175879397) + %\n","tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n","  for item in data:\n","      if any(np.array_equal(item, x) for x in activeL_classical_list):\n","          new_active_classical.append(item)\n","      else:\n","          new_active_non_classical.append(item)\n","\n","        #############################\n","        #         NEW ROUND         #\n","        #     percent = 0.05   #\n","        #############################\n","        \n","\n","        #############################\n","        #    Calculating Ratio     #\n","        #############################\n","        \n","283: 0.9938470721244812\n","738: 0.9938089847564697\n","440: 0.993803858757019\n","729: 0.9937777519226074\n","662: 0.9937707781791687\n","368: 0.9937705397605896\n","677: 0.9937511682510376\n","179: 0.993743360042572\n","446: 0.9937219023704529\n","444: 0.9937146306037903\n","[283, 738, 440, 729, 662, 368, 677, 179, 446, 444]\n","\n","        #############################\n","        #       Begin Training      #\n","        #############################\n","        \n","Training for 1000 epochs on cuda\n","Epoch 1/1000\n","Training loss: 0.91\n","Training accuracy: 8.70\n","\n","Epoch 2/1000\n","Training loss: 0.91\n","Training accuracy: 8.70\n","\n","Epoch 3/1000\n","Training loss: 0.91\n","Training accuracy: 21.74\n","\n","Epoch 4/1000\n","Training loss: 0.91\n","Training accuracy: 26.09\n","\n","Epoch 5/1000\n","Training loss: 0.90\n","Training accuracy: 34.78\n","\n","Epoch 6/1000\n","Training loss: 0.90\n","Training accuracy: 43.48\n","\n","Epoch 7/1000\n","Training loss: 0.90\n","Training accuracy: 34.78\n","\n","Epoch 8/1000\n","Training loss: 0.90\n","Training accuracy: 43.48\n","\n","Epoch 9/1000\n","Training loss: 0.90\n","Training accuracy: 52.17\n","\n","Epoch 10/1000\n","Training loss: 0.90\n","Training accuracy: 56.52\n","\n","Epoch 11/1000\n","Training loss: 0.90\n","Training accuracy: 56.52\n","\n","Epoch 12/1000\n","Training loss: 0.90\n","Training accuracy: 52.17\n","\n","Epoch 13/1000\n","Training loss: 0.90\n","Training accuracy: 56.52\n","\n","Epoch 14/1000\n","Training loss: 0.90\n","Training accuracy: 65.22\n","\n","Epoch 15/1000\n","Training loss: 0.90\n","Training accuracy: 60.87\n","\n","Epoch 16/1000\n","Training loss: 0.90\n","Training accuracy: 82.61\n","\n","Epoch 17/1000\n","Training loss: 0.90\n","Training accuracy: 82.61\n","\n","Epoch 18/1000\n","Training loss: 0.90\n","Training accuracy: 86.96\n","\n","Epoch 19/1000\n","Training loss: 0.90\n","Training accuracy: 91.30\n","\n","Epoch 20/1000\n","Training loss: 0.90\n","Training accuracy: 95.65\n","\n","Epoch 21/1000\n","Training loss: 0.90\n","Training accuracy: 95.65\n","\n","Epoch 22/1000\n","Training loss: 0.90\n","Training accuracy: 95.65\n","\n","Epoch 23/1000\n","Training loss: 0.90\n","Training accuracy: 91.30\n","\n","Epoch 24/1000\n","Training loss: 0.90\n","Training accuracy: 95.65\n","\n","Epoch 25/1000\n","Training loss: 0.90\n","Training accuracy: 95.65\n","\n","Epoch 26/1000\n","Training loss: 0.90\n","Training accuracy: 95.65\n","\n","Epoch 27/1000\n","Training loss: 0.90\n","Training accuracy: 100.00\n","\n","Epoch 28/1000\n","Training loss: 0.90\n","Training accuracy: 100.00\n","\n","Epoch 29/1000\n","Training loss: 0.90\n","Training accuracy: 100.00\n","\n","Epoch 30/1000\n","Training loss: 0.90\n","Training accuracy: 95.65\n","\n","Epoch 31/1000\n","Training loss: 0.90\n","Training accuracy: 100.00\n","\n","Epoch 32/1000\n","Training loss: 0.90\n","Training accuracy: 100.00\n","\n","Epoch 33/1000\n","Training loss: 0.90\n","Training accuracy: 100.00\n","\n","Epoch 34/1000\n","Training loss: 0.90\n","Training accuracy: 95.65\n","\n","Epoch 35/1000\n","Training loss: 0.90\n","Training accuracy: 100.00\n","\n","Epoch 36/1000\n","Training loss: 0.90\n","Training accuracy: 100.00\n","\n","Epoch 37/1000\n","Training loss: 0.90\n","Training accuracy: 100.00\n","\n","Epoch 38/1000\n","Training loss: 0.90\n","Training accuracy: 100.00\n","\n","Epoch 39/1000\n","Training loss: 0.90\n","Training accuracy: 100.00\n","\n","Epoch 40/1000\n","Training loss: 0.90\n","Training accuracy: 100.00\n","\n","Epoch 41/1000\n","Training loss: 0.90\n","Training accuracy: 100.00\n","\n","Epoch 42/1000\n","Training loss: 0.90\n","Training accuracy: 100.00\n","\n","Total training time: 30.6 seconds\n","Ratio_Confidence 0.05 % accuracy :33.165829145728644) + %\n","tensor([0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,\n","        0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0,\n","        0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n","        0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0,\n","        1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,\n","        0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1,\n","        0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 1, 1, 0, 0, 0], device='cuda:0')\n","  for item in data:\n","      if any(np.array_equal(item, x) for x in activeL_classical_list):\n","          new_active_classical.append(item)\n","      else:\n","          new_active_non_classical.append(item)\n","\n","        #############################\n","        #         NEW ROUND         #\n","        #     percent = 0.1   #\n","        #############################\n","        \n","\n","        #############################\n","        #    Calculating Ratio     #\n","        #############################\n","        \n","1: 0.9999983906745911\n","61: 0.9999895691871643\n","265: 0.9999876618385315\n","186: 0.9999628663063049\n","418: 0.9999516010284424\n","248: 0.9999406337738037\n","707: 0.9999339580535889\n","604: 0.9999200701713562\n","199: 0.9999117851257324\n","28: 0.9999098181724548\n","[1, 61, 265, 186, 418, 248, 707, 604, 199, 28]\n","\n","        #############################\n","        #       Begin Training      #\n","        #############################\n","        \n","Training for 1000 epochs on cuda\n","Epoch 1/1000\n","Training loss: 0.73\n","Training accuracy: 28.95\n","\n","Epoch 2/1000\n","Training loss: 0.73\n","Training accuracy: 42.11\n","\n","Epoch 3/1000\n","Training loss: 0.73\n","Training accuracy: 57.89\n","\n","Epoch 4/1000\n","Training loss: 0.73\n","Training accuracy: 63.16\n","\n","Epoch 5/1000\n","Training loss: 0.73\n","Training accuracy: 71.05\n","\n","Epoch 6/1000\n","Training loss: 0.73\n","Training accuracy: 73.68\n","\n","Epoch 7/1000\n","Training loss: 0.73\n","Training accuracy: 86.84\n","\n","Epoch 8/1000\n","Training loss: 0.73\n","Training accuracy: 92.11\n","\n","Epoch 9/1000\n","Training loss: 0.73\n","Training accuracy: 97.37\n","\n","Epoch 10/1000\n","Training loss: 0.73\n","Training accuracy: 97.37\n","\n","Epoch 11/1000\n","Training loss: 0.73\n","Training accuracy: 100.00\n","\n","Epoch 12/1000\n","Training loss: 0.73\n","Training accuracy: 100.00\n","\n","Epoch 13/1000\n","Training loss: 0.73\n","Training accuracy: 100.00\n","\n","Epoch 14/1000\n","Training loss: 0.73\n","Training accuracy: 100.00\n","\n","Epoch 15/1000\n","Training loss: 0.73\n","Training accuracy: 100.00\n","\n","Epoch 16/1000\n","Training loss: 0.73\n","Training accuracy: 100.00\n","\n","Epoch 17/1000\n","Training loss: 0.73\n","Training accuracy: 100.00\n","\n","Epoch 18/1000\n","Training loss: 0.73\n","Training accuracy: 100.00\n","\n","Epoch 19/1000\n","Training loss: 0.73\n","Training accuracy: 100.00\n","\n","Epoch 20/1000\n","Training loss: 0.73\n","Training accuracy: 100.00\n","\n","Epoch 21/1000\n","Training loss: 0.73\n","Training accuracy: 100.00\n","\n","Epoch 22/1000\n","Training loss: 0.73\n","Training accuracy: 100.00\n","\n","Epoch 23/1000\n","Training loss: 0.73\n","Training accuracy: 100.00\n","\n","Epoch 24/1000\n","Training loss: 0.73\n","Training accuracy: 100.00\n","\n","Epoch 25/1000\n","Training loss: 0.73\n","Training accuracy: 100.00\n","\n","Epoch 26/1000\n","Training loss: 0.73\n","Training accuracy: 97.37\n","\n","Epoch 27/1000\n","Training loss: 0.73\n","Training accuracy: 100.00\n","\n","Epoch 28/1000\n","Training loss: 0.73\n","Training accuracy: 100.00\n","\n","Epoch 29/1000\n","Training loss: 0.73\n","Training accuracy: 100.00\n","\n","Epoch 30/1000\n","Training loss: 0.73\n","Training accuracy: 100.00\n","\n","Epoch 31/1000\n","Training loss: 0.73\n","Training accuracy: 100.00\n","\n","Epoch 32/1000\n","Training loss: 0.73\n","Training accuracy: 100.00\n","\n","Total training time: 30.8 seconds\n","Ratio_Confidence 0.1 % accuracy :83.91959798994975) + %\n","tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n","        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,\n","        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n","  for item in data:\n","      if any(np.array_equal(item, x) for x in activeL_classical_list):\n","          new_active_classical.append(item)\n","      else:\n","          new_active_non_classical.append(item)\n","\n","        #############################\n","        #         NEW ROUND         #\n","        #     percent = 0.2   #\n","        #############################\n","        \n","\n","        #############################\n","        #    Calculating Ratio     #\n","        #############################\n","        \n","321: 0.9999768733978271\n","142: 0.9996183514595032\n","6: 0.9986010193824768\n","33: 0.9984048008918762\n","56: 0.9982287287712097\n","144: 0.9978191256523132\n","4: 0.9975360035896301\n","258: 0.9974145889282227\n","587: 0.9973955154418945\n","431: 0.99660325050354\n","[321, 142, 6, 33, 56, 144, 4, 258, 587, 431]\n","\n","        #############################\n","        #       Begin Training      #\n","        #############################\n","        \n","Training for 1000 epochs on cuda\n","Epoch 1/1000\n","Training loss: 0.77\n","Training accuracy: 76.39\n","\n","Epoch 2/1000\n","Training loss: 0.77\n","Training accuracy: 81.94\n","\n","Epoch 3/1000\n","Training loss: 0.77\n","Training accuracy: 84.72\n","\n","Epoch 4/1000\n","Training loss: 0.77\n","Training accuracy: 84.72\n","\n","Epoch 5/1000\n","Training loss: 0.77\n","Training accuracy: 86.11\n","\n","Epoch 6/1000\n","Training loss: 0.77\n","Training accuracy: 94.44\n","\n","Epoch 7/1000\n","Training loss: 0.77\n","Training accuracy: 93.06\n","\n","Epoch 8/1000\n","Training loss: 0.77\n","Training accuracy: 98.61\n","\n","Epoch 9/1000\n","Training loss: 0.77\n","Training accuracy: 98.61\n","\n","Epoch 10/1000\n","Training loss: 0.77\n","Training accuracy: 97.22\n","\n","Epoch 11/1000\n","Training loss: 0.77\n","Training accuracy: 98.61\n","\n","Epoch 12/1000\n","Training loss: 0.77\n","Training accuracy: 97.22\n","\n","Epoch 13/1000\n","Training loss: 0.77\n","Training accuracy: 100.00\n","\n","Epoch 14/1000\n","Training loss: 0.77\n","Training accuracy: 98.61\n","\n","Epoch 15/1000\n","Training loss: 0.77\n","Training accuracy: 100.00\n","\n","Epoch 16/1000\n","Training loss: 0.77\n","Training accuracy: 100.00\n","\n","Epoch 17/1000\n","Training loss: 0.77\n","Training accuracy: 98.61\n","\n","Epoch 18/1000\n","Training loss: 0.77\n","Training accuracy: 98.61\n","\n","Epoch 19/1000\n","Training loss: 0.77\n","Training accuracy: 98.61\n","\n","Epoch 20/1000\n","Training loss: 0.77\n","Training accuracy: 98.61\n","\n","Epoch 21/1000\n","Training loss: 0.77\n","Training accuracy: 100.00\n","\n","Epoch 22/1000\n","Training loss: 0.77\n","Training accuracy: 100.00\n","\n","Total training time: 30.7 seconds\n","Ratio_Confidence 0.2 % accuracy :88.44221105527639) + %\n","tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n","  for item in data:\n","      if any(np.array_equal(item, x) for x in activeL_classical_list):\n","          new_active_classical.append(item)\n","      else:\n","          new_active_non_classical.append(item)\n","\n","        #############################\n","        #         NEW ROUND         #\n","        #     percent = 0.5   #\n","        #############################\n","        \n","\n","        #############################\n","        #    Calculating Ratio     #\n","        #############################\n","        \n","28: 0.9850308895111084\n","251: 0.9842430949211121\n","22: 0.9840361475944519\n","162: 0.9821784496307373\n","25: 0.9819588661193848\n","20: 0.9817976951599121\n","368: 0.9817174077033997\n","197: 0.9816569089889526\n","41: 0.9815900921821594\n","113: 0.9814749956130981\n","[28, 251, 22, 162, 25, 20, 368, 197, 41, 113]\n","\n","        #############################\n","        #       Begin Training      #\n","        #############################\n","        \n","Training for 1000 epochs on cuda\n","Epoch 1/1000\n","Training loss: 0.71\n","Training accuracy: 80.00\n","\n","Epoch 2/1000\n","Training loss: 0.71\n","Training accuracy: 83.08\n","\n","Epoch 3/1000\n","Training loss: 0.71\n","Training accuracy: 90.26\n","\n","Epoch 4/1000\n","Training loss: 0.71\n","Training accuracy: 90.26\n","\n","Epoch 5/1000\n","Training loss: 0.71\n","Training accuracy: 91.79\n","\n","Epoch 6/1000\n","Training loss: 0.71\n","Training accuracy: 93.85\n","\n","Epoch 7/1000\n","Training loss: 0.71\n","Training accuracy: 94.36\n","\n","Epoch 8/1000\n","Training loss: 0.71\n","Training accuracy: 94.87\n","\n","Epoch 9/1000\n","Training loss: 0.71\n","Training accuracy: 95.38\n","\n","Total training time: 32.2 seconds\n","Ratio_Confidence 0.5 % accuracy :91.95979899497488) + %\n","tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n","  for item in data:\n","      if any(np.array_equal(item, x) for x in activeL_classical_list):\n","          new_active_classical.append(item)\n","      else:\n","          new_active_non_classical.append(item)\n","\n","        #############################\n","        #         NEW ROUND         #\n","        #     percent = 0.7   #\n","        #############################\n","        \n","\n","        #############################\n","        #    Calculating Ratio     #\n","        #############################\n","        \n","288: 0.975911557674408\n","88: 0.9751290678977966\n","437: 0.9750340580940247\n","270: 0.9747466444969177\n","243: 0.9747097492218018\n","38: 0.9746975302696228\n","367: 0.9746784567832947\n","90: 0.9746676087379456\n","385: 0.9746573567390442\n","223: 0.9746419191360474\n","[288, 88, 437, 270, 243, 38, 367, 90, 385, 223]\n","\n","        #############################\n","        #       Begin Training      #\n","        #############################\n","        \n","Training for 1000 epochs on cuda\n","Epoch 1/1000\n","Training loss: 0.76\n","Training accuracy: 96.70\n","\n","Epoch 2/1000\n","Training loss: 0.75\n","Training accuracy: 96.70\n","\n","Epoch 3/1000\n","Training loss: 0.75\n","Training accuracy: 96.70\n","\n","Epoch 4/1000\n","Training loss: 0.75\n","Training accuracy: 96.70\n","\n","Epoch 5/1000\n","Training loss: 0.75\n","Training accuracy: 97.80\n","\n","Epoch 6/1000\n","Training loss: 0.75\n","Training accuracy: 97.80\n","\n","Epoch 7/1000\n","Training loss: 0.75\n","Training accuracy: 97.80\n","\n","Epoch 8/1000\n","Training loss: 0.75\n","Training accuracy: 97.80\n","\n","Epoch 9/1000\n","Training loss: 0.75\n","Training accuracy: 98.90\n","\n","Epoch 10/1000\n","Training loss: 0.75\n","Training accuracy: 98.90\n","\n","Epoch 11/1000\n","Training loss: 0.75\n","Training accuracy: 98.90\n","\n","Epoch 12/1000\n","Training loss: 0.75\n","Training accuracy: 100.00\n","\n","Epoch 13/1000\n","Training loss: 0.75\n","Training accuracy: 98.90\n","\n","Epoch 14/1000\n","Training loss: 0.75\n","Training accuracy: 98.90\n","\n","Epoch 15/1000\n","Training loss: 0.75\n","Training accuracy: 98.90\n","\n","Epoch 16/1000\n","Training loss: 0.75\n","Training accuracy: 100.00\n","\n","Epoch 17/1000\n","Training loss: 0.75\n","Training accuracy: 100.00\n","\n","Epoch 18/1000\n","Training loss: 0.75\n","Training accuracy: 100.00\n","\n","Total training time: 30.7 seconds\n","Ratio_Confidence 0.7 % accuracy :92.96482412060301) + %\n","tensor([1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1,\n","        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,\n","        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n","  for item in data:\n","      if any(np.array_equal(item, x) for x in activeL_classical_list):\n","          new_active_classical.append(item)\n","      else:\n","          new_active_non_classical.append(item)\n","\n","        #############################\n","        #         NEW ROUND         #\n","        #     percent = 1.0   #\n","        #############################\n","        \n","\n","        #############################\n","        #    Calculating Ratio     #\n","        #############################\n","        \n","17: 0.9674388766288757\n","6: 0.9668086171150208\n","333: 0.9667276740074158\n","12: 0.9667099714279175\n","41: 0.9666517972946167\n","0: 0.9666181802749634\n","176: 0.9666035771369934\n","112: 0.966600239276886\n","5: 0.9665864706039429\n","130: 0.9665853977203369\n","[17, 6, 333, 12, 41, 0, 176, 112, 5, 130]\n","\n","        #############################\n","        #       Begin Training      #\n","        #############################\n","        \n","Training for 1000 epochs on cuda\n","Epoch 1/1000\n","Training loss: 0.69\n","Training accuracy: 88.99\n","\n","Epoch 2/1000\n","Training loss: 0.69\n","Training accuracy: 92.66\n","\n","Epoch 3/1000\n","Training loss: 0.69\n","Training accuracy: 92.66\n","\n","Epoch 4/1000\n","Training loss: 0.69\n","Training accuracy: 92.66\n","\n","Epoch 5/1000\n","Training loss: 0.69\n","Training accuracy: 95.41\n","\n","Epoch 6/1000\n","Training loss: 0.69\n","Training accuracy: 92.66\n","\n","Epoch 7/1000\n","Training loss: 0.69\n","Training accuracy: 96.33\n","\n","Epoch 8/1000\n","Training loss: 0.69\n","Training accuracy: 96.33\n","\n","Epoch 9/1000\n","Training loss: 0.69\n","Training accuracy: 95.41\n","\n","Epoch 10/1000\n","Training loss: 0.69\n","Training accuracy: 96.33\n","\n","Epoch 11/1000\n","Training loss: 0.69\n","Training accuracy: 96.33\n","\n","Epoch 12/1000\n","Training loss: 0.69\n","Training accuracy: 96.33\n","\n","Epoch 13/1000\n","Training loss: 0.69\n","Training accuracy: 96.33\n","\n","Epoch 14/1000\n","Training loss: 0.69\n","Training accuracy: 96.33\n","\n","Epoch 15/1000\n","Training loss: 0.69\n","Training accuracy: 96.33\n","\n","Total training time: 30.5 seconds\n","Ratio_Confidence 1.0 % accuracy :92.96482412060301) + %\n","tensor([1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 0], device='cuda:0')\n"]},{"output_type":"execute_result","data":{"text/plain":["   Percentage                    Accuracy\n","0        0.02   (14.07035175879397, 1000)\n","1        0.05  (33.165829145728644, 1000)\n","2        0.10   (83.91959798994975, 1000)\n","3        0.20   (88.44221105527639, 1000)\n","4        0.50   (91.95979899497488, 1000)\n","5        0.70   (92.96482412060301, 1000)\n","6        1.00   (92.96482412060301, 1000)"],"text/html":["\n","  <div id=\"df-f5ebea74-c69f-4dc9-8092-7f2edf0c3463\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Percentage</th>\n","      <th>Accuracy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.02</td>\n","      <td>(14.07035175879397, 1000)</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.05</td>\n","      <td>(33.165829145728644, 1000)</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.10</td>\n","      <td>(83.91959798994975, 1000)</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.20</td>\n","      <td>(88.44221105527639, 1000)</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.50</td>\n","      <td>(91.95979899497488, 1000)</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>0.70</td>\n","      <td>(92.96482412060301, 1000)</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>1.00</td>\n","      <td>(92.96482412060301, 1000)</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f5ebea74-c69f-4dc9-8092-7f2edf0c3463')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-f5ebea74-c69f-4dc9-8092-7f2edf0c3463 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-f5ebea74-c69f-4dc9-8092-7f2edf0c3463');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-d160bc5b-c62b-440e-9986-0486ff4b130e\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-d160bc5b-c62b-440e-9986-0486ff4b130e')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-d160bc5b-c62b-440e-9986-0486ff4b130e button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":5}]}]}