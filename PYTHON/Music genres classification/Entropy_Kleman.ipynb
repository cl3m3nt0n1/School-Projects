{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyMaSspe5e6FKM34ZlvXqAnS"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","from urllib.request import urlopen\n","from PIL import Image\n","import torch\n","from transformers import AutoFeatureExtractor, ResNetForImageClassification, ResNetModel\n","import os\n","from PIL import Image\n","from torch.utils.data import TensorDataset, DataLoader\n","import time\n","import numpy as np\n","device = 'cuda'\n","from transformers import ResNetModel\n","import torch\n","\n","def train(net, train_dataloader, criterion, optimizer, scheduler=None, epochs=100, device=device, checkpoint_epochs=2, timeout=45):\n","    start = time.time()\n","    print(f'Training for {epochs} epochs on {device}')\n","\n","    for epoch in range(1,epochs+1):\n","        print(f\"Epoch {epoch}/{epochs}\")\n","\n","        net.train()  # put network in train mode for Dropout and Batch Normalization\n","        train_loss = torch.tensor(0., device=device)  # loss and accuracy tensors are on the GPU to avoid data transfers\n","        train_accuracy = torch.tensor(0., device=device)\n","        for X, y in train_dataloader:\n","            X = X.to(device)\n","            y = y.type(torch.LongTensor).to(device)\n","            preds = net(X)\n","            loss = criterion(preds, y)\n","\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","            with torch.no_grad():\n","                train_loss += loss * train_dataloader.batch_size\n","                train_accuracy += (torch.argmax(preds, dim=1) == y).sum()\n","\n","        if scheduler is not None:\n","            scheduler.step()\n","\n","        print(f'Training loss: {train_loss/len(train_dataloader.dataset):.2f}')\n","        print(f'Training accuracy: {100*train_accuracy/len(train_dataloader.dataset):.2f}')\n","\n","\n","        if epoch%checkpoint_epochs==0:\n","            torch.save({\n","                'epoch': epoch,\n","                'state_dict': net.state_dict(),\n","                'optimizer': optimizer.state_dict(),\n","            }, './checkpoint.pth.tar')\n","\n","        print()\n","        if((time.time() - start) >= timeout):\n","          break\n","\n","    end = time.time()\n","    print(f'Total training time: {end-start:.1f} seconds')\n","    return net\n","\n","# model definition\n","class Classifier_model(torch.nn.Module):\n","    # define model elements\n","    def __init__(self):\n","        super(Classifier_model, self).__init__()\n","        self.device = device\n","        self.image_processor = AutoFeatureExtractor.from_pretrained(\"microsoft/resnet-18\",device=self.device)\n","        self.pre_trained_model = ResNetModel.from_pretrained(\"microsoft/resnet-18\")\n","        resnet18_output_size=25088\n","        self.fc = torch.nn.Linear(resnet18_output_size, 10)\n","        self.activation = torch.nn.ReLU()\n","\n","    # forward propagate input\n","    def forward(self, X):\n","        X = self.image_processor(X, return_tensors=\"pt\").to(self.device)\n","        # print(X.pixel_value.is_cuda)\n","        X = self.pre_trained_model(**X).last_hidden_state.flatten(start_dim=1)\n","        X = self.activation(X)\n","        X = self.fc(X)\n","\n","        return X.softmax(dim=1)\n","\n","    def features_extractor(self, X):\n","        X = self.image_processor(X, return_tensors=\"pt\").to(self.device)\n","        # print(X.pixel_value.is_cuda)\n","        X = self.pre_trained_model(**X).last_hidden_state.flatten(start_dim=1)\n","\n","        return X.softmax(dim=1)\n","\n","# model definition\n","class Classifier_model_2(torch.nn.Module):\n","    # define model elements\n","    def __init__(self,f_model):\n","        super(Classifier_model_2, self).__init__()\n","        self.device = device\n","        resnet18_output_size=25088\n","        self.f_model = f_model\n","        self.fc = torch.nn.Linear(resnet18_output_size, 2)\n","        self.activation = torch.nn.ReLU()\n","\n","    # forward propagate input\n","    def forward(self, X):\n","        #X = self.image_processor(X, return_tensors=\"pt\").to(self.device)\n","        # print(X.pixel_value.is_cuda)\n","        X = self.f_model.features_extractor(X)\n","        X = self.activation(X)\n","        X = self.fc(X)\n","\n","        return X.softmax(dim=1)\n","\n","\n","# Chemin du répertoire racine\n","import random\n","root_dir = '/content/drive/MyDrive/ColabNotebooks/mel'\n","\n","# Dictionnaire pour stocker les images par sous-dossier\n","image_dict = {}\n","\n","# Parcourir tous les sous-dossiers\n","for root, dirs, files in os.walk(root_dir):\n","    for file in files:\n","        # Construire le chemin complet du fichier\n","        file_path = os.path.join(root, file)\n","\n","        # Vérifier si le fichier est une image en fonction de l'extension (par exemple, .png)\n","        if file_path.lower().endswith(('.png', '.jpg', '.jpeg', '.gif')):\n","            # Ouvrir l'image avec Pillow\n","            image = Image.open(file_path)\n","\n","            # Convertir l'image en format JPG (si elle n'est pas déjà en JPG)\n","            if image.format != \"JPEG\":\n","                image = image.convert(\"RGB\")\n","\n","            # Obtenez le nom du sous-dossier parent\n","            parent_dir = os.path.basename(os.path.dirname(file_path))\n","\n","            # Vérifiez si le sous-dossier existe dans le dictionnaire, sinon créez-le\n","            if parent_dir not in image_dict:\n","                image_dict[parent_dir] = []\n","\n","            # Ajouter l'image à la liste du sous-dossier correspondant\n","            image = image.resize((336,219))\n","            image_dict[parent_dir].append(np.array(image))\n","\n","classical_list_MG = image_dict['classic']\n","non_classical_list_MG = image_dict['non_classic']\n","\n","percent_classical_MG = int(0.2*len(classical_list_MG))\n","percent_non_classical_MG = int(0.2*len(non_classical_list_MG))\n","\n","\n","evaluation_list_dic = []\n","for sample in classical_list_MG[:percent_classical_MG]:\n","    buffer = []\n","    buffer.append(sample)\n","    buffer.append(0)\n","    evaluation_list_dic.append(buffer)\n","\n","for sample in non_classical_list_MG[:percent_non_classical_MG]:\n","    buffer = []\n","    buffer.append(sample)\n","    buffer.append(1)\n","    evaluation_list_dic.append(buffer)\n","\n","random.shuffle(evaluation_list_dic)\n","evaluation_list = []\n","for sample in evaluation_list_dic:\n","    evaluation_list.append(sample[0])\n","\n","activeL_classical_list = classical_list_MG[percent_classical_MG:]\n","activeL_non_classical_list = non_classical_list_MG[percent_non_classical_MG:]\n"],"metadata":{"id":"09mt6poN8zpx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1702995440221,"user_tz":-60,"elapsed":45321,"user":{"displayName":"Clemzi","userId":"17076433745448136774"}},"outputId":"5ff1aece-3740-40ec-fb30-b9828760fe0b"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["def EntropyDataLoader(percent, previousPercent, uncertainties, dataset):\n","  \"\"\"\n","  This function is used after having determined the Entropy Confidence of the base dataset.\n","  Keywords:\n","    percent         : The percentage of the base dataset to take account of\n","    previousPercent : The previous percentage that was used the last time this function was called\n","    uncertainties   : A dictionnary containing the uncertainties associated with each sample of dataset\n","    dataset         : The updated dataset containing only unseen samples\n","  Returns:\n","    my_dataloader   : pytorch compatible data corresponding to the dataset used for the model training\n","    dataset         : The new dataset containing unseen samples\n","  \"\"\"\n","\n","  # How many samples do I have to care about this time ?\n","  difference_in_percent = percent - previousPercent\n","  lastPoint_uncertainties = int(difference_in_percent * len(uncertainties))\n","\n","  # Taking only that many samples into consideration\n","  samples_to_take = list(uncertainties.keys())[:lastPoint_uncertainties]\n","  print(samples_to_take[:10])\n","  # the indices inside the dataset of the most uncertained samples\n","  data = []\n","  for i in range(len(samples_to_take)):\n","    data.append(dataset[samples_to_take[i]])\n","\n","  dataset = np.delete(dataset, samples_to_take, axis=0)\n","\n","  # Now, we need to recreate two tensors knowing if each samples contained\n","  # inside the dataset is classical or not.\n","  new_active_classical = []\n","  new_active_non_classical = []\n","  for item in data:\n","      if any(np.array_equal(item, x) for x in activeL_classical_list):\n","          new_active_classical.append(item)\n","      else:\n","          new_active_non_classical.append(item)\n","\n","  if (len(new_active_classical) == 0):\n","    tensor_x = torch.Tensor(new_active_non_classical).to(device)\n","    tensor_y = torch.Tensor(np.full(len(new_active_non_classical),  1)).to(device)\n","\n","\n","  else:\n","    tensor_x = torch.Tensor(np.concatenate((new_active_classical, new_active_non_classical), axis = 0)).to(device)\n","    tensor_y = torch.Tensor(np.concatenate((np.full(len(new_active_classical), 0),\n","                                        np.full(len(new_active_non_classical),  1)), axis=0)).to(device)\n","\n","\n","  my_dataset = TensorDataset(tensor_x,tensor_y) # create your datset\n","  my_dataloader = DataLoader(my_dataset,batch_size=10, shuffle=True)\n","  return my_dataloader, dataset"],"metadata":{"id":"PXpyRHTiD4pX","executionInfo":{"status":"ok","timestamp":1702995440223,"user_tz":-60,"elapsed":68,"user":{"displayName":"Clemzi","userId":"17076433745448136774"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["from scipy.stats import entropy\n","\n","def EntropyConfidence(model, test_data):\n","  \"\"\"\n","  Keywords:\n","    model : The model that gives predictions\n","    data  : The dataset to base the predictions on\n","    n     : the number of best candidates to select\n","\n","  Returns:\n","    A sorted dictionnary containing the uncertainty score associated with a sample\n","  \"\"\"\n","  uncertainty_dict = {}\n","  for i in range(len(test_data)):\n","    preds = model(test_data[i])\n","    uncertainty_dict[i] = entropy(preds[0].cpu().detach().numpy())\n","  res = dict(sorted(uncertainty_dict.items(),\n","                    key = lambda x: x[1], reverse = True))\n","  for key in list(res.keys())[:10]:\n","      print(f\"{key}: {res[key]}\")\n","  return res"],"metadata":{"id":"rNbQ6GVnD-14","executionInfo":{"status":"ok","timestamp":1702995440549,"user_tz":-60,"elapsed":389,"user":{"displayName":"Clemzi","userId":"17076433745448136774"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","execution_count":6,"metadata":{"id":"St5iHgGYfZCn","executionInfo":{"status":"ok","timestamp":1702995549158,"user_tz":-60,"elapsed":417,"user":{"displayName":"Clemzi","userId":"17076433745448136774"}}},"outputs":[],"source":["import os\n","import random\n","import numpy as np\n","import pandas as pd\n","import torch\n","from torch.utils.data import DataLoader, TensorDataset\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","\n","def entropyConfidence_Train(modelToTrain, evaluation_list,save_dir='/content/drive/MyDrive/ColabNotebooks/models/EntropyConfidence'):\n","\n","    # Créer un répertoire s'il n'existe pas pour y stocker les modèles\n","    if not os.path.exists(save_dir):\n","        os.makedirs(save_dir)\n","\n","    results = []\n","\n","    # Pourcentages du dataset à sélectionner\n","    percentages = [0.02, 0.05,0.10, 0.20, 0.50, 0.70, 1.00]\n","    previousPercent = 0\n","\n","    dataset = np.concatenate((activeL_classical_list, activeL_non_classical_list))\n","\n","    for percent in percentages:\n","        print(f\"\"\"  for item in data:\n","      if any(np.array_equal(item, x) for x in activeL_classical_list):\n","          new_active_classical.append(item)\n","      else:\n","          new_active_non_classical.append(item)\n","\n","        #############################\n","        #         NEW ROUND         #\n","        #     percent = {percent}   #\n","        #############################\n","        \"\"\")\n","        lr, weight_decay, epochs,timeout = 1e-5, 5e-4, 1000, 30\n","        net = torch.load(modelToTrain)\n","        loss = torch.nn.CrossEntropyLoss()\n","        optimizer = torch.optim.Adam(net.parameters(),lr=lr, weight_decay=weight_decay)\n","\n","        # Active Learning\n","        print(f\"\"\"\n","        #############################\n","        #    Calculating Entropy    #\n","        #############################\n","        \"\"\")\n","        uncertainties = EntropyConfidence(net, dataset)\n","        dataloader, dataset = EntropyDataLoader(percent, previousPercent, uncertainties, dataset)\n","\n","        print(f\"\"\"\n","        #############################\n","        #       Begin Training      #\n","        #############################\n","        \"\"\")\n","\n","        net2 = train(net, dataloader, loss, optimizer, None, epochs, device, timeout=timeout)\n","        torch.save(net2, \"/content/drive/MyDrive/ColabNotebooks/models/EntropyConfidence/entropy_confidence_\" + str(percent) + \"_percent.pt\")\n","\n","\n","\n","        # Faire des prédictions sur la liste d'évaluation\n","        model = net2\n","        model.eval()\n","        test_data=torch.Tensor(np.array(evaluation_list)).to(device)\n","        with torch.no_grad():\n","          preds = model(test_data)\n","        output=torch.argmax(preds, dim=1)\n","\n","        # Evaluation du modèle\n","        count = 0\n","        for i in range(0,len(output)):\n","            if output[i].item() == evaluation_list_dic[i][1]:\n","                count+=1\n","        print(f\"Entropy_Confidence {percent} % accuracy :{(count/len(output))*100}) + %\")\n","        print(output)\n","\n","        accuracy = (count/len(output))*100, epochs\n","\n","        # Ajouter les résultats à la liste\n","        results.append([percent, accuracy])\n","        previousPercent = percent\n","        # Utiliser le modèle précédent pour poursuivre l'entraînement\n","        modelToTrain = \"/content/drive/MyDrive/ColabNotebooks/models/EntropyConfidence/entropy_confidence_\" + str(percent) + \"_percent.pt\"\n","\n","    # Créer un DataFrame à partir des résultats\n","    results_df = pd.DataFrame(results, columns=['Percentage', 'Accuracy'])\n","\n","    # Sauvegarder les résultats dans un fichier CSV\n","    results_csv_path = os.path.join(save_dir, 'results.csv')\n","    results_df.to_csv(results_csv_path, index=False)\n","\n","    return results_df\n"]},{"cell_type":"code","source":["entropyConfidence_Train(\"/content/drive/MyDrive/ColabNotebooks/model_2_Clem_trainings_longEgale_Colab.pt\", evaluation_list)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"gZDpRn5eehl_","executionInfo":{"status":"ok","timestamp":1702995866022,"user_tz":-60,"elapsed":315387,"user":{"displayName":"Clemzi","userId":"17076433745448136774"}},"outputId":"8125215d-d8b0-4b19-e6fa-f5607a35ccae"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["  for item in data:\n","      if any(np.array_equal(item, x) for x in activeL_classical_list):\n","          new_active_classical.append(item)\n","      else:\n","          new_active_non_classical.append(item)\n","\n","        #############################\n","        #         NEW ROUND         #\n","        #     percent = 0.02   #\n","        #############################\n","        \n","\n","        #############################\n","        #    Calculating Entropy    #\n","        #############################\n","        \n","337: 0.6931381225585938\n","124: 0.6931380033493042\n","372: 0.6931378841400146\n","467: 0.6931378841400146\n","339: 0.6931378245353699\n","554: 0.6931377649307251\n","585: 0.6931377649307251\n","271: 0.6931376457214355\n","537: 0.6931376457214355\n","572: 0.6931376457214355\n","[337, 124, 372, 467, 339, 554, 585, 271, 537, 572]\n","\n","        #############################\n","        #       Begin Training      #\n","        #############################\n","        \n","Training for 1000 epochs on cuda\n","Epoch 1/1000\n","Training loss: 0.87\n","Training accuracy: 0.00\n","\n","Epoch 2/1000\n","Training loss: 0.87\n","Training accuracy: 0.00\n","\n","Epoch 3/1000\n","Training loss: 0.87\n","Training accuracy: 0.00\n","\n","Epoch 4/1000\n","Training loss: 0.87\n","Training accuracy: 6.25\n","\n","Epoch 5/1000\n","Training loss: 0.87\n","Training accuracy: 6.25\n","\n","Epoch 6/1000\n","Training loss: 0.87\n","Training accuracy: 12.50\n","\n","Epoch 7/1000\n","Training loss: 0.87\n","Training accuracy: 6.25\n","\n","Epoch 8/1000\n","Training loss: 0.87\n","Training accuracy: 12.50\n","\n","Epoch 9/1000\n","Training loss: 0.87\n","Training accuracy: 6.25\n","\n","Epoch 10/1000\n","Training loss: 0.87\n","Training accuracy: 31.25\n","\n","Epoch 11/1000\n","Training loss: 0.87\n","Training accuracy: 31.25\n","\n","Epoch 12/1000\n","Training loss: 0.87\n","Training accuracy: 25.00\n","\n","Epoch 13/1000\n","Training loss: 0.87\n","Training accuracy: 37.50\n","\n","Epoch 14/1000\n","Training loss: 0.87\n","Training accuracy: 50.00\n","\n","Epoch 15/1000\n","Training loss: 0.87\n","Training accuracy: 37.50\n","\n","Epoch 16/1000\n","Training loss: 0.87\n","Training accuracy: 43.75\n","\n","Epoch 17/1000\n","Training loss: 0.87\n","Training accuracy: 43.75\n","\n","Epoch 18/1000\n","Training loss: 0.87\n","Training accuracy: 50.00\n","\n","Epoch 19/1000\n","Training loss: 0.87\n","Training accuracy: 43.75\n","\n","Epoch 20/1000\n","Training loss: 0.87\n","Training accuracy: 50.00\n","\n","Epoch 21/1000\n","Training loss: 0.87\n","Training accuracy: 56.25\n","\n","Epoch 22/1000\n","Training loss: 0.87\n","Training accuracy: 50.00\n","\n","Epoch 23/1000\n","Training loss: 0.87\n","Training accuracy: 43.75\n","\n","Epoch 24/1000\n","Training loss: 0.87\n","Training accuracy: 50.00\n","\n","Epoch 25/1000\n","Training loss: 0.87\n","Training accuracy: 62.50\n","\n","Epoch 26/1000\n","Training loss: 0.87\n","Training accuracy: 62.50\n","\n","Epoch 27/1000\n","Training loss: 0.87\n","Training accuracy: 68.75\n","\n","Epoch 28/1000\n","Training loss: 0.87\n","Training accuracy: 62.50\n","\n","Epoch 29/1000\n","Training loss: 0.87\n","Training accuracy: 62.50\n","\n","Epoch 30/1000\n","Training loss: 0.87\n","Training accuracy: 68.75\n","\n","Epoch 31/1000\n","Training loss: 0.87\n","Training accuracy: 68.75\n","\n","Epoch 32/1000\n","Training loss: 0.87\n","Training accuracy: 68.75\n","\n","Epoch 33/1000\n","Training loss: 0.87\n","Training accuracy: 75.00\n","\n","Epoch 34/1000\n","Training loss: 0.87\n","Training accuracy: 75.00\n","\n","Epoch 35/1000\n","Training loss: 0.87\n","Training accuracy: 81.25\n","\n","Epoch 36/1000\n","Training loss: 0.87\n","Training accuracy: 81.25\n","\n","Epoch 37/1000\n","Training loss: 0.87\n","Training accuracy: 81.25\n","\n","Epoch 38/1000\n","Training loss: 0.87\n","Training accuracy: 81.25\n","\n","Epoch 39/1000\n","Training loss: 0.87\n","Training accuracy: 81.25\n","\n","Epoch 40/1000\n","Training loss: 0.87\n","Training accuracy: 81.25\n","\n","Epoch 41/1000\n","Training loss: 0.87\n","Training accuracy: 81.25\n","\n","Epoch 42/1000\n","Training loss: 0.87\n","Training accuracy: 81.25\n","\n","Epoch 43/1000\n","Training loss: 0.87\n","Training accuracy: 87.50\n","\n","Epoch 44/1000\n","Training loss: 0.87\n","Training accuracy: 87.50\n","\n","Epoch 45/1000\n","Training loss: 0.87\n","Training accuracy: 87.50\n","\n","Epoch 46/1000\n","Training loss: 0.87\n","Training accuracy: 81.25\n","\n","Total training time: 32.6 seconds\n","Entropy_Confidence 0.02 % accuracy :14.572864321608039) + %\n","tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n","        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 1, 0, 0, 0, 0], device='cuda:0')\n","  for item in data:\n","      if any(np.array_equal(item, x) for x in activeL_classical_list):\n","          new_active_classical.append(item)\n","      else:\n","          new_active_non_classical.append(item)\n","\n","        #############################\n","        #         NEW ROUND         #\n","        #     percent = 0.05   #\n","        #############################\n","        \n","\n","        #############################\n","        #    Calculating Entropy    #\n","        #############################\n","        \n","677: 0.693142831325531\n","738: 0.6931427717208862\n","152: 0.6931427121162415\n","781: 0.6931427121162415\n","179: 0.6931426525115967\n","546: 0.6931426525115967\n","283: 0.6931425929069519\n","141: 0.6931425333023071\n","217: 0.6931425333023071\n","251: 0.6931425333023071\n","[677, 738, 152, 781, 179, 546, 283, 141, 217, 251]\n","\n","        #############################\n","        #       Begin Training      #\n","        #############################\n","        \n","Training for 1000 epochs on cuda\n","Epoch 1/1000\n","Training loss: 0.91\n","Training accuracy: 0.00\n","\n","Epoch 2/1000\n","Training loss: 0.91\n","Training accuracy: 0.00\n","\n","Epoch 3/1000\n","Training loss: 0.91\n","Training accuracy: 8.70\n","\n","Epoch 4/1000\n","Training loss: 0.91\n","Training accuracy: 8.70\n","\n","Epoch 5/1000\n","Training loss: 0.91\n","Training accuracy: 17.39\n","\n","Epoch 6/1000\n","Training loss: 0.90\n","Training accuracy: 26.09\n","\n","Epoch 7/1000\n","Training loss: 0.90\n","Training accuracy: 26.09\n","\n","Epoch 8/1000\n","Training loss: 0.90\n","Training accuracy: 43.48\n","\n","Epoch 9/1000\n","Training loss: 0.90\n","Training accuracy: 52.17\n","\n","Epoch 10/1000\n","Training loss: 0.90\n","Training accuracy: 56.52\n","\n","Epoch 11/1000\n","Training loss: 0.90\n","Training accuracy: 65.22\n","\n","Epoch 12/1000\n","Training loss: 0.90\n","Training accuracy: 52.17\n","\n","Epoch 13/1000\n","Training loss: 0.90\n","Training accuracy: 56.52\n","\n","Epoch 14/1000\n","Training loss: 0.90\n","Training accuracy: 60.87\n","\n","Epoch 15/1000\n","Training loss: 0.90\n","Training accuracy: 69.57\n","\n","Epoch 16/1000\n","Training loss: 0.90\n","Training accuracy: 65.22\n","\n","Epoch 17/1000\n","Training loss: 0.90\n","Training accuracy: 65.22\n","\n","Epoch 18/1000\n","Training loss: 0.90\n","Training accuracy: 78.26\n","\n","Epoch 19/1000\n","Training loss: 0.90\n","Training accuracy: 73.91\n","\n","Epoch 20/1000\n","Training loss: 0.90\n","Training accuracy: 86.96\n","\n","Epoch 21/1000\n","Training loss: 0.90\n","Training accuracy: 86.96\n","\n","Epoch 22/1000\n","Training loss: 0.90\n","Training accuracy: 82.61\n","\n","Epoch 23/1000\n","Training loss: 0.90\n","Training accuracy: 82.61\n","\n","Epoch 24/1000\n","Training loss: 0.90\n","Training accuracy: 86.96\n","\n","Epoch 25/1000\n","Training loss: 0.90\n","Training accuracy: 86.96\n","\n","Epoch 26/1000\n","Training loss: 0.90\n","Training accuracy: 91.30\n","\n","Epoch 27/1000\n","Training loss: 0.90\n","Training accuracy: 95.65\n","\n","Epoch 28/1000\n","Training loss: 0.90\n","Training accuracy: 95.65\n","\n","Epoch 29/1000\n","Training loss: 0.90\n","Training accuracy: 95.65\n","\n","Epoch 30/1000\n","Training loss: 0.90\n","Training accuracy: 95.65\n","\n","Epoch 31/1000\n","Training loss: 0.90\n","Training accuracy: 100.00\n","\n","Epoch 32/1000\n","Training loss: 0.90\n","Training accuracy: 95.65\n","\n","Epoch 33/1000\n","Training loss: 0.90\n","Training accuracy: 100.00\n","\n","Epoch 34/1000\n","Training loss: 0.90\n","Training accuracy: 95.65\n","\n","Epoch 35/1000\n","Training loss: 0.90\n","Training accuracy: 100.00\n","\n","Epoch 36/1000\n","Training loss: 0.90\n","Training accuracy: 100.00\n","\n","Epoch 37/1000\n","Training loss: 0.90\n","Training accuracy: 100.00\n","\n","Epoch 38/1000\n","Training loss: 0.90\n","Training accuracy: 100.00\n","\n","Epoch 39/1000\n","Training loss: 0.90\n","Training accuracy: 100.00\n","\n","Epoch 40/1000\n","Training loss: 0.90\n","Training accuracy: 100.00\n","\n","Epoch 41/1000\n","Training loss: 0.90\n","Training accuracy: 95.65\n","\n","Epoch 42/1000\n","Training loss: 0.90\n","Training accuracy: 100.00\n","\n","Total training time: 31.1 seconds\n","Entropy_Confidence 0.05 % accuracy :36.68341708542713) + %\n","tensor([0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0,\n","        0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0,\n","        0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0,\n","        1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,\n","        0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0,\n","        0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1,\n","        0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1,\n","        0, 1, 0, 0, 0, 1, 0], device='cuda:0')\n","  for item in data:\n","      if any(np.array_equal(item, x) for x in activeL_classical_list):\n","          new_active_classical.append(item)\n","      else:\n","          new_active_non_classical.append(item)\n","\n","        #############################\n","        #         NEW ROUND         #\n","        #     percent = 0.1   #\n","        #############################\n","        \n","\n","        #############################\n","        #    Calculating Entropy    #\n","        #############################\n","        \n","14: 0.6931471824645996\n","22: 0.6931471824645996\n","98: 0.6931471824645996\n","111: 0.6931471824645996\n","123: 0.6931471824645996\n","134: 0.6931471824645996\n","163: 0.6931471824645996\n","173: 0.6931471824645996\n","187: 0.6931471824645996\n","191: 0.6931471824645996\n","[14, 22, 98, 111, 123, 134, 163, 173, 187, 191]\n","\n","        #############################\n","        #       Begin Training      #\n","        #############################\n","        \n","Training for 1000 epochs on cuda\n","Epoch 1/1000\n","Training loss: 0.73\n","Training accuracy: 44.74\n","\n","Epoch 2/1000\n","Training loss: 0.73\n","Training accuracy: 52.63\n","\n","Epoch 3/1000\n","Training loss: 0.73\n","Training accuracy: 76.32\n","\n","Epoch 4/1000\n","Training loss: 0.73\n","Training accuracy: 76.32\n","\n","Epoch 5/1000\n","Training loss: 0.73\n","Training accuracy: 81.58\n","\n","Epoch 6/1000\n","Training loss: 0.73\n","Training accuracy: 86.84\n","\n","Epoch 7/1000\n","Training loss: 0.73\n","Training accuracy: 89.47\n","\n","Epoch 8/1000\n","Training loss: 0.73\n","Training accuracy: 94.74\n","\n","Epoch 9/1000\n","Training loss: 0.73\n","Training accuracy: 94.74\n","\n","Epoch 10/1000\n","Training loss: 0.73\n","Training accuracy: 94.74\n","\n","Epoch 11/1000\n","Training loss: 0.73\n","Training accuracy: 97.37\n","\n","Epoch 12/1000\n","Training loss: 0.73\n","Training accuracy: 94.74\n","\n","Epoch 13/1000\n","Training loss: 0.73\n","Training accuracy: 97.37\n","\n","Epoch 14/1000\n","Training loss: 0.73\n","Training accuracy: 97.37\n","\n","Epoch 15/1000\n","Training loss: 0.73\n","Training accuracy: 97.37\n","\n","Epoch 16/1000\n","Training loss: 0.73\n","Training accuracy: 97.37\n","\n","Epoch 17/1000\n","Training loss: 0.73\n","Training accuracy: 97.37\n","\n","Epoch 18/1000\n","Training loss: 0.73\n","Training accuracy: 100.00\n","\n","Epoch 19/1000\n","Training loss: 0.73\n","Training accuracy: 100.00\n","\n","Epoch 20/1000\n","Training loss: 0.73\n","Training accuracy: 100.00\n","\n","Epoch 21/1000\n","Training loss: 0.73\n","Training accuracy: 100.00\n","\n","Epoch 22/1000\n","Training loss: 0.73\n","Training accuracy: 100.00\n","\n","Epoch 23/1000\n","Training loss: 0.73\n","Training accuracy: 100.00\n","\n","Epoch 24/1000\n","Training loss: 0.73\n","Training accuracy: 100.00\n","\n","Epoch 25/1000\n","Training loss: 0.73\n","Training accuracy: 100.00\n","\n","Epoch 26/1000\n","Training loss: 0.73\n","Training accuracy: 100.00\n","\n","Epoch 27/1000\n","Training loss: 0.73\n","Training accuracy: 100.00\n","\n","Epoch 28/1000\n","Training loss: 0.73\n","Training accuracy: 100.00\n","\n","Epoch 29/1000\n","Training loss: 0.73\n","Training accuracy: 100.00\n","\n","Epoch 30/1000\n","Training loss: 0.73\n","Training accuracy: 100.00\n","\n","Epoch 31/1000\n","Training loss: 0.73\n","Training accuracy: 100.00\n","\n","Epoch 32/1000\n","Training loss: 0.73\n","Training accuracy: 100.00\n","\n","Epoch 33/1000\n","Training loss: 0.73\n","Training accuracy: 100.00\n","\n","Epoch 34/1000\n","Training loss: 0.73\n","Training accuracy: 100.00\n","\n","Epoch 35/1000\n","Training loss: 0.73\n","Training accuracy: 100.00\n","\n","Epoch 36/1000\n","Training loss: 0.73\n","Training accuracy: 100.00\n","\n","Total training time: 30.7 seconds\n","Entropy_Confidence 0.1 % accuracy :86.4321608040201) + %\n","tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n","  for item in data:\n","      if any(np.array_equal(item, x) for x in activeL_classical_list):\n","          new_active_classical.append(item)\n","      else:\n","          new_active_non_classical.append(item)\n","\n","        #############################\n","        #         NEW ROUND         #\n","        #     percent = 0.2   #\n","        #############################\n","        \n","\n","        #############################\n","        #    Calculating Entropy    #\n","        #############################\n","        \n","146: 0.6931471824645996\n","5: 0.6931463479995728\n","34: 0.693146288394928\n","3: 0.6931456327438354\n","303: 0.6931456327438354\n","58: 0.6931453943252563\n","7: 0.6931448578834534\n","255: 0.693144679069519\n","317: 0.693144679069519\n","166: 0.6931445598602295\n","[146, 5, 34, 3, 303, 58, 7, 255, 317, 166]\n","\n","        #############################\n","        #       Begin Training      #\n","        #############################\n","        \n","Training for 1000 epochs on cuda\n","Epoch 1/1000\n","Training loss: 0.77\n","Training accuracy: 68.06\n","\n","Epoch 2/1000\n","Training loss: 0.77\n","Training accuracy: 83.33\n","\n","Epoch 3/1000\n","Training loss: 0.77\n","Training accuracy: 91.67\n","\n","Epoch 4/1000\n","Training loss: 0.77\n","Training accuracy: 88.89\n","\n","Epoch 5/1000\n","Training loss: 0.77\n","Training accuracy: 95.83\n","\n","Epoch 6/1000\n","Training loss: 0.77\n","Training accuracy: 91.67\n","\n","Epoch 7/1000\n","Training loss: 0.77\n","Training accuracy: 94.44\n","\n","Epoch 8/1000\n","Training loss: 0.77\n","Training accuracy: 94.44\n","\n","Epoch 9/1000\n","Training loss: 0.77\n","Training accuracy: 94.44\n","\n","Epoch 10/1000\n","Training loss: 0.77\n","Training accuracy: 94.44\n","\n","Epoch 11/1000\n","Training loss: 0.77\n","Training accuracy: 97.22\n","\n","Epoch 12/1000\n","Training loss: 0.77\n","Training accuracy: 95.83\n","\n","Epoch 13/1000\n","Training loss: 0.77\n","Training accuracy: 98.61\n","\n","Epoch 14/1000\n","Training loss: 0.77\n","Training accuracy: 95.83\n","\n","Epoch 15/1000\n","Training loss: 0.77\n","Training accuracy: 97.22\n","\n","Epoch 16/1000\n","Training loss: 0.77\n","Training accuracy: 95.83\n","\n","Epoch 17/1000\n","Training loss: 0.77\n","Training accuracy: 98.61\n","\n","Epoch 18/1000\n","Training loss: 0.77\n","Training accuracy: 95.83\n","\n","Epoch 19/1000\n","Training loss: 0.77\n","Training accuracy: 98.61\n","\n","Epoch 20/1000\n","Training loss: 0.77\n","Training accuracy: 98.61\n","\n","Epoch 21/1000\n","Training loss: 0.77\n","Training accuracy: 97.22\n","\n","Epoch 22/1000\n","Training loss: 0.76\n","Training accuracy: 98.61\n","\n","Epoch 23/1000\n","Training loss: 0.76\n","Training accuracy: 98.61\n","\n","Epoch 24/1000\n","Training loss: 0.77\n","Training accuracy: 95.83\n","\n","Total training time: 30.5 seconds\n","Entropy_Confidence 0.2 % accuracy :84.92462311557789) + %\n","tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1,\n","        1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n","        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,\n","        1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n","  for item in data:\n","      if any(np.array_equal(item, x) for x in activeL_classical_list):\n","          new_active_classical.append(item)\n","      else:\n","          new_active_non_classical.append(item)\n","\n","        #############################\n","        #         NEW ROUND         #\n","        #     percent = 0.5   #\n","        #############################\n","        \n","\n","        #############################\n","        #    Calculating Entropy    #\n","        #############################\n","        \n","239: 0.6931471824645996\n","11: 0.6931444406509399\n","3: 0.6931437849998474\n","266: 0.6931411623954773\n","30: 0.693140983581543\n","33: 0.6931408643722534\n","8: 0.6931302547454834\n","50: 0.6931301355361938\n","365: 0.6931296586990356\n","55: 0.6931282877922058\n","[239, 11, 3, 266, 30, 33, 8, 50, 365, 55]\n","\n","        #############################\n","        #       Begin Training      #\n","        #############################\n","        \n","Training for 1000 epochs on cuda\n","Epoch 1/1000\n","Training loss: 0.71\n","Training accuracy: 78.97\n","\n","Epoch 2/1000\n","Training loss: 0.71\n","Training accuracy: 81.54\n","\n","Epoch 3/1000\n","Training loss: 0.71\n","Training accuracy: 86.67\n","\n","Epoch 4/1000\n","Training loss: 0.71\n","Training accuracy: 90.26\n","\n","Epoch 5/1000\n","Training loss: 0.71\n","Training accuracy: 92.31\n","\n","Epoch 6/1000\n","Training loss: 0.71\n","Training accuracy: 92.82\n","\n","Epoch 7/1000\n","Training loss: 0.71\n","Training accuracy: 96.41\n","\n","Epoch 8/1000\n","Training loss: 0.71\n","Training accuracy: 95.38\n","\n","Epoch 9/1000\n","Training loss: 0.70\n","Training accuracy: 97.95\n","\n","Epoch 10/1000\n","Training loss: 0.70\n","Training accuracy: 97.95\n","\n","Total training time: 32.7 seconds\n","Entropy_Confidence 0.5 % accuracy :93.96984924623115) + %\n","tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 0, 1, 1, 1, 1, 1], device='cuda:0')\n","  for item in data:\n","      if any(np.array_equal(item, x) for x in activeL_classical_list):\n","          new_active_classical.append(item)\n","      else:\n","          new_active_non_classical.append(item)\n","\n","        #############################\n","        #         NEW ROUND         #\n","        #     percent = 0.7   #\n","        #############################\n","        \n","\n","        #############################\n","        #    Calculating Entropy    #\n","        #############################\n","        \n","36: 0.6930721998214722\n","215: 0.6930699348449707\n","0: 0.69306880235672\n","277: 0.6930683851242065\n","369: 0.6930657625198364\n","385: 0.6930654644966125\n","398: 0.6930644512176514\n","423: 0.6930638551712036\n","409: 0.693063497543335\n","99: 0.6930623054504395\n","[36, 215, 0, 277, 369, 385, 398, 423, 409, 99]\n","\n","        #############################\n","        #       Begin Training      #\n","        #############################\n","        \n","Training for 1000 epochs on cuda\n","Epoch 1/1000\n","Training loss: 0.75\n","Training accuracy: 93.41\n","\n","Epoch 2/1000\n","Training loss: 0.75\n","Training accuracy: 95.60\n","\n","Epoch 3/1000\n","Training loss: 0.76\n","Training accuracy: 95.60\n","\n","Epoch 4/1000\n","Training loss: 0.75\n","Training accuracy: 98.90\n","\n","Epoch 5/1000\n","Training loss: 0.75\n","Training accuracy: 98.90\n","\n","Epoch 6/1000\n","Training loss: 0.75\n","Training accuracy: 97.80\n","\n","Epoch 7/1000\n","Training loss: 0.75\n","Training accuracy: 98.90\n","\n","Epoch 8/1000\n","Training loss: 0.75\n","Training accuracy: 98.90\n","\n","Epoch 9/1000\n","Training loss: 0.75\n","Training accuracy: 96.70\n","\n","Epoch 10/1000\n","Training loss: 0.75\n","Training accuracy: 97.80\n","\n","Epoch 11/1000\n","Training loss: 0.75\n","Training accuracy: 98.90\n","\n","Epoch 12/1000\n","Training loss: 0.75\n","Training accuracy: 98.90\n","\n","Epoch 13/1000\n","Training loss: 0.75\n","Training accuracy: 98.90\n","\n","Epoch 14/1000\n","Training loss: 0.75\n","Training accuracy: 97.80\n","\n","Epoch 15/1000\n","Training loss: 0.75\n","Training accuracy: 98.90\n","\n","Epoch 16/1000\n","Training loss: 0.75\n","Training accuracy: 98.90\n","\n","Epoch 17/1000\n","Training loss: 0.75\n","Training accuracy: 98.90\n","\n","Epoch 18/1000\n","Training loss: 0.75\n","Training accuracy: 98.90\n","\n","Epoch 19/1000\n","Training loss: 0.75\n","Training accuracy: 98.90\n","\n","Epoch 20/1000\n","Training loss: 0.75\n","Training accuracy: 98.90\n","\n","Total training time: 31.2 seconds\n","Entropy_Confidence 0.7 % accuracy :90.95477386934674) + %\n","tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 0, 1, 1, 1, 1, 1], device='cuda:0')\n","  for item in data:\n","      if any(np.array_equal(item, x) for x in activeL_classical_list):\n","          new_active_classical.append(item)\n","      else:\n","          new_active_non_classical.append(item)\n","\n","        #############################\n","        #         NEW ROUND         #\n","        #     percent = 1.0   #\n","        #############################\n","        \n","\n","        #############################\n","        #    Calculating Entropy    #\n","        #############################\n","        \n","35: 0.6929857730865479\n","38: 0.6929852962493896\n","42: 0.6929852962493896\n","130: 0.6929851770401001\n","132: 0.6929851770401001\n","163: 0.6929844617843628\n","336: 0.6929842233657837\n","116: 0.6929836869239807\n","0: 0.6929811239242554\n","47: 0.6929807662963867\n","[35, 38, 42, 130, 132, 163, 336, 116, 0, 47]\n","\n","        #############################\n","        #       Begin Training      #\n","        #############################\n","        \n","Training for 1000 epochs on cuda\n","Epoch 1/1000\n","Training loss: 0.69\n","Training accuracy: 96.33\n","\n","Epoch 2/1000\n","Training loss: 0.69\n","Training accuracy: 96.33\n","\n","Epoch 3/1000\n","Training loss: 0.69\n","Training accuracy: 96.33\n","\n","Epoch 4/1000\n","Training loss: 0.69\n","Training accuracy: 96.33\n","\n","Epoch 5/1000\n","Training loss: 0.69\n","Training accuracy: 96.33\n","\n","Epoch 6/1000\n","Training loss: 0.69\n","Training accuracy: 96.33\n","\n","Epoch 7/1000\n","Training loss: 0.69\n","Training accuracy: 96.33\n","\n","Epoch 8/1000\n","Training loss: 0.69\n","Training accuracy: 96.33\n","\n","Epoch 9/1000\n","Training loss: 0.69\n","Training accuracy: 96.33\n","\n","Epoch 10/1000\n","Training loss: 0.69\n","Training accuracy: 96.33\n","\n","Epoch 11/1000\n","Training loss: 0.69\n","Training accuracy: 96.33\n","\n","Epoch 12/1000\n","Training loss: 0.69\n","Training accuracy: 96.33\n","\n","Epoch 13/1000\n","Training loss: 0.69\n","Training accuracy: 96.33\n","\n","Epoch 14/1000\n","Training loss: 0.69\n","Training accuracy: 96.33\n","\n","Epoch 15/1000\n","Training loss: 0.69\n","Training accuracy: 96.33\n","\n","Epoch 16/1000\n","Training loss: 0.69\n","Training accuracy: 96.33\n","\n","Total training time: 30.7 seconds\n","Entropy_Confidence 1.0 % accuracy :89.9497487437186) + %\n","tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n"]},{"output_type":"execute_result","data":{"text/plain":["   Percentage                    Accuracy\n","0        0.02  (14.572864321608039, 1000)\n","1        0.05   (36.68341708542713, 1000)\n","2        0.10    (86.4321608040201, 1000)\n","3        0.20   (84.92462311557789, 1000)\n","4        0.50   (93.96984924623115, 1000)\n","5        0.70   (90.95477386934674, 1000)\n","6        1.00    (89.9497487437186, 1000)"],"text/html":["\n","  <div id=\"df-12ce3a59-e6d8-4a21-b938-e2bcab7e5c5f\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Percentage</th>\n","      <th>Accuracy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.02</td>\n","      <td>(14.572864321608039, 1000)</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.05</td>\n","      <td>(36.68341708542713, 1000)</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.10</td>\n","      <td>(86.4321608040201, 1000)</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.20</td>\n","      <td>(84.92462311557789, 1000)</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.50</td>\n","      <td>(93.96984924623115, 1000)</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>0.70</td>\n","      <td>(90.95477386934674, 1000)</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>1.00</td>\n","      <td>(89.9497487437186, 1000)</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-12ce3a59-e6d8-4a21-b938-e2bcab7e5c5f')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-12ce3a59-e6d8-4a21-b938-e2bcab7e5c5f button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-12ce3a59-e6d8-4a21-b938-e2bcab7e5c5f');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-d3ac743f-d8ef-4dd1-bf96-7900aff2deec\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-d3ac743f-d8ef-4dd1-bf96-7900aff2deec')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-d3ac743f-d8ef-4dd1-bf96-7900aff2deec button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":7}]}]}