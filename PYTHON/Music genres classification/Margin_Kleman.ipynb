{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyNqQ3Z6OJbN35UgpetahULh"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","from urllib.request import urlopen\n","from PIL import Image\n","import torch\n","from transformers import AutoFeatureExtractor, ResNetForImageClassification, ResNetModel\n","import os\n","from PIL import Image\n","from torch.utils.data import TensorDataset, DataLoader\n","import time\n","import numpy as np\n","device = 'cuda'\n","from transformers import ResNetModel\n","import torch\n","\n","def train(net, train_dataloader, criterion, optimizer, scheduler=None, epochs=100, device=device, checkpoint_epochs=2, timeout=45):\n","    start = time.time()\n","    print(f'Training for {epochs} epochs on {device}')\n","\n","    for epoch in range(1,epochs+1):\n","        print(f\"Epoch {epoch}/{epochs}\")\n","\n","        net.train()  # put network in train mode for Dropout and Batch Normalization\n","        train_loss = torch.tensor(0., device=device)  # loss and accuracy tensors are on the GPU to avoid data transfers\n","        train_accuracy = torch.tensor(0., device=device)\n","        for X, y in train_dataloader:\n","            X = X.to(device)\n","            y = y.type(torch.LongTensor).to(device)\n","            preds = net(X)\n","            loss = criterion(preds, y)\n","\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","            with torch.no_grad():\n","                train_loss += loss * train_dataloader.batch_size\n","                train_accuracy += (torch.argmax(preds, dim=1) == y).sum()\n","\n","        if scheduler is not None:\n","            scheduler.step()\n","\n","        print(f'Training loss: {train_loss/len(train_dataloader.dataset):.2f}')\n","        print(f'Training accuracy: {100*train_accuracy/len(train_dataloader.dataset):.2f}')\n","\n","\n","        if epoch%checkpoint_epochs==0:\n","            torch.save({\n","                'epoch': epoch,\n","                'state_dict': net.state_dict(),\n","                'optimizer': optimizer.state_dict(),\n","            }, './checkpoint.pth.tar')\n","\n","        print()\n","        if((time.time() - start) >= timeout):\n","          break\n","\n","    end = time.time()\n","    print(f'Total training time: {end-start:.1f} seconds')\n","    return net\n","\n","# model definition\n","class Classifier_model(torch.nn.Module):\n","    # define model elements\n","    def __init__(self):\n","        super(Classifier_model, self).__init__()\n","        self.device = device\n","        self.image_processor = AutoFeatureExtractor.from_pretrained(\"microsoft/resnet-18\",device=self.device)\n","        self.pre_trained_model = ResNetModel.from_pretrained(\"microsoft/resnet-18\")\n","        resnet18_output_size=25088\n","        self.fc = torch.nn.Linear(resnet18_output_size, 10)\n","        self.activation = torch.nn.ReLU()\n","\n","    # forward propagate input\n","    def forward(self, X):\n","        X = self.image_processor(X, return_tensors=\"pt\").to(self.device)\n","        # print(X.pixel_value.is_cuda)\n","        X = self.pre_trained_model(**X).last_hidden_state.flatten(start_dim=1)\n","        X = self.activation(X)\n","        X = self.fc(X)\n","\n","        return X.softmax(dim=1)\n","\n","    def features_extractor(self, X):\n","        X = self.image_processor(X, return_tensors=\"pt\").to(self.device)\n","        # print(X.pixel_value.is_cuda)\n","        X = self.pre_trained_model(**X).last_hidden_state.flatten(start_dim=1)\n","\n","        return X.softmax(dim=1)\n","\n","# model definition\n","class Classifier_model_2(torch.nn.Module):\n","    # define model elements\n","    def __init__(self,f_model):\n","        super(Classifier_model_2, self).__init__()\n","        self.device = device\n","        resnet18_output_size=25088\n","        self.f_model = f_model\n","        self.fc = torch.nn.Linear(resnet18_output_size, 2)\n","        self.activation = torch.nn.ReLU()\n","\n","    # forward propagate input\n","    def forward(self, X):\n","        #X = self.image_processor(X, return_tensors=\"pt\").to(self.device)\n","        # print(X.pixel_value.is_cuda)\n","        X = self.f_model.features_extractor(X)\n","        X = self.activation(X)\n","        X = self.fc(X)\n","\n","        return X.softmax(dim=1)\n","\n","\n","# Chemin du répertoire racine\n","import random\n","root_dir = '/content/drive/MyDrive/ColabNotebooks/mel'\n","\n","# Dictionnaire pour stocker les images par sous-dossier\n","image_dict = {}\n","\n","# Parcourir tous les sous-dossiers\n","for root, dirs, files in os.walk(root_dir):\n","    for file in files:\n","        # Construire le chemin complet du fichier\n","        file_path = os.path.join(root, file)\n","\n","        # Vérifier si le fichier est une image en fonction de l'extension (par exemple, .png)\n","        if file_path.lower().endswith(('.png', '.jpg', '.jpeg', '.gif')):\n","            # Ouvrir l'image avec Pillow\n","            image = Image.open(file_path)\n","\n","            # Convertir l'image en format JPG (si elle n'est pas déjà en JPG)\n","            if image.format != \"JPEG\":\n","                image = image.convert(\"RGB\")\n","\n","            # Obtenez le nom du sous-dossier parent\n","            parent_dir = os.path.basename(os.path.dirname(file_path))\n","\n","            # Vérifiez si le sous-dossier existe dans le dictionnaire, sinon créez-le\n","            if parent_dir not in image_dict:\n","                image_dict[parent_dir] = []\n","\n","            # Ajouter l'image à la liste du sous-dossier correspondant\n","            image = image.resize((336,219))\n","            image_dict[parent_dir].append(np.array(image))\n","\n","classical_list_MG = image_dict['classic']\n","non_classical_list_MG = image_dict['non_classic']\n","\n","percent_classical_MG = int(0.2*len(classical_list_MG))\n","percent_non_classical_MG = int(0.2*len(non_classical_list_MG))\n","\n","\n","evaluation_list_dic = []\n","for sample in classical_list_MG[:percent_classical_MG]:\n","    buffer = []\n","    buffer.append(sample)\n","    buffer.append(0)\n","    evaluation_list_dic.append(buffer)\n","\n","for sample in non_classical_list_MG[:percent_non_classical_MG]:\n","    buffer = []\n","    buffer.append(sample)\n","    buffer.append(1)\n","    evaluation_list_dic.append(buffer)\n","\n","random.shuffle(evaluation_list_dic)\n","evaluation_list = []\n","for sample in evaluation_list_dic:\n","    evaluation_list.append(sample[0])\n","\n","activeL_classical_list = classical_list_MG[percent_classical_MG:]\n","activeL_non_classical_list = non_classical_list_MG[percent_non_classical_MG:]\n"],"metadata":{"id":"09mt6poN8zpx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1702986572418,"user_tz":-60,"elapsed":11197,"user":{"displayName":"Clemzi","userId":"17076433745448136774"}},"outputId":"c2e79c9f-3524-4fe3-ebc4-069694cedbb1"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["# Margin Of Confidence\n","  # Prendre le full dataset\n","  # Avec le baseline, faire les prédictions\n","  # Une fois que les prédictions sont faites, Margin of Confidence\n","  # Grâce à Margin of Confidence, on obtient le dataset trié par intérêt.\n","  # On prend alors percent % de ce dataset trié et on entraîne le modèle.\n","  # On supprime ensuite ces samples du dataset original et on réitère pour le nouveau modèle obtenu.\n","\n","# Fonctions à (ré)écrire :\n","  # Active Dataloader : Doit fournir les données sélectionnées par Margin Of Confidence au modèle pour l'entraînement\n","  # MarginOfConfidence : Calcule la margin of confidence à partir d'un dataset donné\n","  # prepareData : Prépare un nouveau dataset en fonction du pourcentage à voir pour le modèle\n"],"metadata":{"id":"81WGm1_29KZ2","executionInfo":{"status":"ok","timestamp":1702986572421,"user_tz":-60,"elapsed":24,"user":{"displayName":"Clemzi","userId":"17076433745448136774"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["def MarginDataLoader(percent, previousPercent, uncertainties, dataset):\n","  \"\"\"\n","  This function is used after having determined the Margin of Confidence of the base dataset.\n","  Keywords:\n","    percent         : The percentage of the base dataset to take account of\n","    previousPercent : The previous percentage that was used the last time this function was called\n","    uncertainties   : A dictionnary containing the uncertainties associated with each sample of dataset\n","    dataset         : The updated dataset containing only unseen samples\n","  Returns:\n","    my_dataloader   : pytorch compatible data corresponding to the dataset used for the model training\n","    dataset         : The new dataset containing unseen samples\n","  \"\"\"\n","\n","  # How many samples do I have to care about this time ?\n","  difference_in_percent = percent - previousPercent\n","  lastPoint_uncertainties = int(difference_in_percent * len(uncertainties))\n","\n","  # Taking only that many samples into consideration\n","  samples_to_take = list(uncertainties.keys())[:lastPoint_uncertainties]\n","  print(samples_to_take[:10])\n","  # the indices inside the dataset of the most uncertained samples\n","  data = []\n","  for i in range(len(samples_to_take)):\n","    data.append(dataset[samples_to_take[i]])\n","\n","  dataset = np.delete(dataset, samples_to_take, axis=0)\n","\n","  # Now, we need to recreate two tensors knowing if each samples contained\n","  # inside the dataset is classical or not.\n","  new_active_classical = []\n","  new_active_non_classical = []\n","  for item in data:\n","      if any(np.array_equal(item, x) for x in activeL_classical_list):\n","          new_active_classical.append(item)\n","      else:\n","          new_active_non_classical.append(item)\n","\n","  if (len(new_active_classical) == 0):\n","    tensor_x = torch.Tensor(new_active_non_classical).to(device)\n","    tensor_y = torch.Tensor(np.full(len(new_active_non_classical),  1)).to(device)\n","\n","\n","  else:\n","    tensor_x = torch.Tensor(np.concatenate((new_active_classical, new_active_non_classical), axis = 0)).to(device)\n","    tensor_y = torch.Tensor(np.concatenate((np.full(len(new_active_classical), 0),\n","                                        np.full(len(new_active_non_classical),  1)), axis=0)).to(device)\n","\n","\n","  my_dataset = TensorDataset(tensor_x,tensor_y) # create your datset\n","  my_dataloader = DataLoader(my_dataset,batch_size=10, shuffle=True)\n","  return my_dataloader, dataset"],"metadata":{"id":"PXpyRHTiD4pX","executionInfo":{"status":"ok","timestamp":1702986572422,"user_tz":-60,"elapsed":21,"user":{"displayName":"Clemzi","userId":"17076433745448136774"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["def MarginOfConfidence(model, test_data):\n","  \"\"\"\n","  Keywords:\n","    model : The model that gives predictions\n","    data  : The dataset to base the predictions on\n","    n     : the number of best candidates to select\n","\n","  Returns:\n","    A sorted dictionnary containing the uncertainty score associated with a sample\n","  \"\"\"\n","  uncertainty_dict = {}\n","  for i in range(len(test_data)):\n","    preds = model(test_data[i])\n","    preds_sorted = np.sort(preds[0].cpu().detach().numpy())\n","    uncertainty_dict[i] = (preds_sorted[0] - preds_sorted[1])\n","  res = dict(sorted(uncertainty_dict.items(),\n","                    key = lambda x: x[1], reverse = True))\n","  for key in list(res.keys())[:10]:\n","      print(f\"{key}: {res[key]}\")\n","  return res"],"metadata":{"id":"rNbQ6GVnD-14","executionInfo":{"status":"ok","timestamp":1702986572423,"user_tz":-60,"elapsed":20,"user":{"displayName":"Clemzi","userId":"17076433745448136774"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","execution_count":7,"metadata":{"id":"St5iHgGYfZCn","executionInfo":{"status":"ok","timestamp":1702986574050,"user_tz":-60,"elapsed":1643,"user":{"displayName":"Clemzi","userId":"17076433745448136774"}}},"outputs":[],"source":["import os\n","import random\n","import numpy as np\n","import pandas as pd\n","import torch\n","from torch.utils.data import DataLoader, TensorDataset\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","\n","def marginOfConfidence_Train(modelToTrain, evaluation_list,save_dir='/content/drive/MyDrive/ColabNotebooks/models/MarginOfConfidence'):\n","\n","    # Créer un répertoire s'il n'existe pas pour y stocker les modèles\n","    if not os.path.exists(save_dir):\n","        os.makedirs(save_dir)\n","\n","    results = []\n","\n","    # Pourcentages du dataset à sélectionner\n","    percentages = [0.02, 0.05,0.10, 0.20, 0.50, 0.70, 1.00]\n","    previousPercent = 0\n","\n","    dataset = np.concatenate((activeL_classical_list, activeL_non_classical_list))\n","\n","    for percent in percentages:\n","        print(f\"\"\"  for item in data:\n","      if any(np.array_equal(item, x) for x in activeL_classical_list):\n","          new_active_classical.append(item)\n","      else:\n","          new_active_non_classical.append(item)\n","\n","        #############################\n","        #         NEW ROUND         #\n","        #     percent = {percent}   #\n","        #############################\n","        \"\"\")\n","        lr, weight_decay, epochs,timeout = 1e-5, 5e-4, 1000, 30\n","        net = torch.load(modelToTrain)\n","        loss = torch.nn.CrossEntropyLoss()\n","        optimizer = torch.optim.Adam(net.parameters(),lr=lr, weight_decay=weight_decay)\n","\n","        # Active Learning\n","        print(f\"\"\"\n","        #############################\n","        #    Calculating Margin     #\n","        #############################\n","        \"\"\")\n","        uncertainties = MarginOfConfidence(net, dataset)\n","        dataloader, dataset = MarginDataLoader(percent, previousPercent, uncertainties, dataset)\n","\n","        print(f\"\"\"\n","        #############################\n","        #       Begin Training      #\n","        #############################\n","        \"\"\")\n","\n","        net2 = train(net, dataloader, loss, optimizer, None, epochs, device, timeout=timeout)\n","        torch.save(net2, \"/content/drive/MyDrive/ColabNotebooks/models/MarginOfConfidence/margin_confidence_\" + str(percent) + \"_percent.pt\")\n","\n","\n","\n","        # Faire des prédictions sur la liste d'évaluation\n","        model = net2\n","        model.eval()\n","        test_data=torch.Tensor(np.array(evaluation_list)).to(device)\n","        with torch.no_grad():\n","          preds = model(test_data)\n","        output=torch.argmax(preds, dim=1)\n","\n","        # Evaluation du modèle\n","        count = 0\n","        for i in range(0,len(output)):\n","            if output[i].item() == evaluation_list_dic[i][1]:\n","                count+=1\n","        print(f\"Margin_Confidence {percent} % accuracy :{(count/len(output))*100}) + %\")\n","        print(output)\n","\n","        accuracy = (count/len(output))*100, epochs\n","\n","        # Ajouter les résultats à la liste\n","        results.append([percent, accuracy])\n","        previousPercent = percent\n","        # Utiliser le modèle précédent pour poursuivre l'entraînement\n","        modelToTrain = \"/content/drive/MyDrive/ColabNotebooks/models/MarginOfConfidence/margin_confidence_\" + str(percent) + \"_percent.pt\"\n","\n","    # Créer un DataFrame à partir des résultats\n","    results_df = pd.DataFrame(results, columns=['Percentage', 'Accuracy'])\n","\n","    # Sauvegarder les résultats dans un fichier CSV\n","    results_csv_path = os.path.join(save_dir, 'results.csv')\n","    results_df.to_csv(results_csv_path, index=False)\n","\n","    return results_df\n"]},{"cell_type":"code","source":["marginOfConfidence_Train(\"/content/drive/MyDrive/ColabNotebooks/model_2_Clem_trainings_longEgale_Colab.pt\", evaluation_list)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"gZDpRn5eehl_","executionInfo":{"status":"ok","timestamp":1702986908955,"user_tz":-60,"elapsed":334919,"user":{"displayName":"Clemzi","userId":"17076433745448136774"}},"outputId":"342a3bcf-d385-4b52-d2a2-6d97f5d4d270"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["  for item in data:\n","      if any(np.array_equal(item, x) for x in activeL_classical_list):\n","          new_active_classical.append(item)\n","      else:\n","          new_active_non_classical.append(item)\n","\n","        #############################\n","        #         NEW ROUND         #\n","        #     percent = 0.02   #\n","        #############################\n","        \n","\n","        #############################\n","        #    Calculating Margin     #\n","        #############################\n","        \n","337: -0.00426483154296875\n","124: -0.004273176193237305\n","372: -0.004311740398406982\n","467: -0.004317253828048706\n","339: -0.004326611757278442\n","554: -0.00432974100112915\n","585: -0.004334062337875366\n","271: -0.004354298114776611\n","619: -0.00436064600944519\n","692: -0.004361659288406372\n","[337, 124, 372, 467, 339, 554, 585, 271, 619, 692]\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-5-5ea00844611e>:39: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:261.)\n","  tensor_x = torch.Tensor(new_active_non_classical).to(device)\n"]},{"output_type":"stream","name":"stdout","text":["\n","        #############################\n","        #       Begin Training      #\n","        #############################\n","        \n","Training for 1000 epochs on cuda\n","Epoch 1/1000\n","Training loss: 0.87\n","Training accuracy: 0.00\n","\n","Epoch 2/1000\n","Training loss: 0.87\n","Training accuracy: 0.00\n","\n","Epoch 3/1000\n","Training loss: 0.87\n","Training accuracy: 0.00\n","\n","Epoch 4/1000\n","Training loss: 0.87\n","Training accuracy: 0.00\n","\n","Epoch 5/1000\n","Training loss: 0.87\n","Training accuracy: 6.25\n","\n","Epoch 6/1000\n","Training loss: 0.87\n","Training accuracy: 12.50\n","\n","Epoch 7/1000\n","Training loss: 0.87\n","Training accuracy: 18.75\n","\n","Epoch 8/1000\n","Training loss: 0.87\n","Training accuracy: 6.25\n","\n","Epoch 9/1000\n","Training loss: 0.87\n","Training accuracy: 18.75\n","\n","Epoch 10/1000\n","Training loss: 0.87\n","Training accuracy: 12.50\n","\n","Epoch 11/1000\n","Training loss: 0.87\n","Training accuracy: 18.75\n","\n","Epoch 12/1000\n","Training loss: 0.87\n","Training accuracy: 25.00\n","\n","Epoch 13/1000\n","Training loss: 0.87\n","Training accuracy: 37.50\n","\n","Epoch 14/1000\n","Training loss: 0.87\n","Training accuracy: 37.50\n","\n","Epoch 15/1000\n","Training loss: 0.87\n","Training accuracy: 43.75\n","\n","Epoch 16/1000\n","Training loss: 0.87\n","Training accuracy: 43.75\n","\n","Epoch 17/1000\n","Training loss: 0.87\n","Training accuracy: 43.75\n","\n","Epoch 18/1000\n","Training loss: 0.87\n","Training accuracy: 43.75\n","\n","Epoch 19/1000\n","Training loss: 0.87\n","Training accuracy: 43.75\n","\n","Epoch 20/1000\n","Training loss: 0.87\n","Training accuracy: 43.75\n","\n","Epoch 21/1000\n","Training loss: 0.87\n","Training accuracy: 50.00\n","\n","Epoch 22/1000\n","Training loss: 0.87\n","Training accuracy: 62.50\n","\n","Epoch 23/1000\n","Training loss: 0.87\n","Training accuracy: 62.50\n","\n","Epoch 24/1000\n","Training loss: 0.87\n","Training accuracy: 68.75\n","\n","Epoch 25/1000\n","Training loss: 0.87\n","Training accuracy: 62.50\n","\n","Epoch 26/1000\n","Training loss: 0.87\n","Training accuracy: 68.75\n","\n","Epoch 27/1000\n","Training loss: 0.87\n","Training accuracy: 75.00\n","\n","Epoch 28/1000\n","Training loss: 0.87\n","Training accuracy: 75.00\n","\n","Epoch 29/1000\n","Training loss: 0.87\n","Training accuracy: 75.00\n","\n","Epoch 30/1000\n","Training loss: 0.87\n","Training accuracy: 75.00\n","\n","Epoch 31/1000\n","Training loss: 0.87\n","Training accuracy: 75.00\n","\n","Epoch 32/1000\n","Training loss: 0.87\n","Training accuracy: 75.00\n","\n","Epoch 33/1000\n","Training loss: 0.87\n","Training accuracy: 75.00\n","\n","Epoch 34/1000\n","Training loss: 0.87\n","Training accuracy: 81.25\n","\n","Epoch 35/1000\n","Training loss: 0.87\n","Training accuracy: 75.00\n","\n","Epoch 36/1000\n","Training loss: 0.87\n","Training accuracy: 81.25\n","\n","Epoch 37/1000\n","Training loss: 0.87\n","Training accuracy: 81.25\n","\n","Epoch 38/1000\n","Training loss: 0.87\n","Training accuracy: 81.25\n","\n","Epoch 39/1000\n","Training loss: 0.87\n","Training accuracy: 87.50\n","\n","Epoch 40/1000\n","Training loss: 0.87\n","Training accuracy: 87.50\n","\n","Epoch 41/1000\n","Training loss: 0.87\n","Training accuracy: 87.50\n","\n","Epoch 42/1000\n","Training loss: 0.87\n","Training accuracy: 87.50\n","\n","Epoch 43/1000\n","Training loss: 0.87\n","Training accuracy: 87.50\n","\n","Epoch 44/1000\n","Training loss: 0.87\n","Training accuracy: 87.50\n","\n","Total training time: 32.3 seconds\n","Margin_Confidence 0.02 % accuracy :13.06532663316583) + %\n","tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n","        0, 0, 0, 0, 0, 0, 1], device='cuda:0')\n","  for item in data:\n","      if any(np.array_equal(item, x) for x in activeL_classical_list):\n","          new_active_classical.append(item)\n","      else:\n","          new_active_non_classical.append(item)\n","\n","        #############################\n","        #         NEW ROUND         #\n","        #     percent = 0.05   #\n","        #############################\n","        \n","\n","        #############################\n","        #    Calculating Margin     #\n","        #############################\n","        \n","283: -0.003011852502822876\n","677: -0.0030249953269958496\n","738: -0.0030364692211151123\n","479: -0.003048539161682129\n","181: -0.0030564069747924805\n","446: -0.003057330846786499\n","179: -0.0030747950077056885\n","440: -0.0030791759490966797\n","546: -0.003088623285293579\n","781: -0.0030955970287323\n","[283, 677, 738, 479, 181, 446, 179, 440, 546, 781]\n","\n","        #############################\n","        #       Begin Training      #\n","        #############################\n","        \n","Training for 1000 epochs on cuda\n","Epoch 1/1000\n","Training loss: 0.91\n","Training accuracy: 4.35\n","\n","Epoch 2/1000\n","Training loss: 0.91\n","Training accuracy: 17.39\n","\n","Epoch 3/1000\n","Training loss: 0.91\n","Training accuracy: 13.04\n","\n","Epoch 4/1000\n","Training loss: 0.91\n","Training accuracy: 21.74\n","\n","Epoch 5/1000\n","Training loss: 0.90\n","Training accuracy: 34.78\n","\n","Epoch 6/1000\n","Training loss: 0.90\n","Training accuracy: 39.13\n","\n","Epoch 7/1000\n","Training loss: 0.90\n","Training accuracy: 34.78\n","\n","Epoch 8/1000\n","Training loss: 0.90\n","Training accuracy: 39.13\n","\n","Epoch 9/1000\n","Training loss: 0.90\n","Training accuracy: 43.48\n","\n","Epoch 10/1000\n","Training loss: 0.90\n","Training accuracy: 43.48\n","\n","Epoch 11/1000\n","Training loss: 0.90\n","Training accuracy: 65.22\n","\n","Epoch 12/1000\n","Training loss: 0.90\n","Training accuracy: 60.87\n","\n","Epoch 13/1000\n","Training loss: 0.90\n","Training accuracy: 73.91\n","\n","Epoch 14/1000\n","Training loss: 0.90\n","Training accuracy: 73.91\n","\n","Epoch 15/1000\n","Training loss: 0.90\n","Training accuracy: 73.91\n","\n","Epoch 16/1000\n","Training loss: 0.90\n","Training accuracy: 78.26\n","\n","Epoch 17/1000\n","Training loss: 0.90\n","Training accuracy: 82.61\n","\n","Epoch 18/1000\n","Training loss: 0.90\n","Training accuracy: 78.26\n","\n","Epoch 19/1000\n","Training loss: 0.90\n","Training accuracy: 86.96\n","\n","Epoch 20/1000\n","Training loss: 0.90\n","Training accuracy: 82.61\n","\n","Epoch 21/1000\n","Training loss: 0.90\n","Training accuracy: 86.96\n","\n","Epoch 22/1000\n","Training loss: 0.90\n","Training accuracy: 82.61\n","\n","Epoch 23/1000\n","Training loss: 0.90\n","Training accuracy: 82.61\n","\n","Epoch 24/1000\n","Training loss: 0.90\n","Training accuracy: 91.30\n","\n","Epoch 25/1000\n","Training loss: 0.90\n","Training accuracy: 86.96\n","\n","Epoch 26/1000\n","Training loss: 0.90\n","Training accuracy: 95.65\n","\n","Epoch 27/1000\n","Training loss: 0.90\n","Training accuracy: 91.30\n","\n","Epoch 28/1000\n","Training loss: 0.90\n","Training accuracy: 86.96\n","\n","Epoch 29/1000\n","Training loss: 0.90\n","Training accuracy: 95.65\n","\n","Epoch 30/1000\n","Training loss: 0.90\n","Training accuracy: 91.30\n","\n","Epoch 31/1000\n","Training loss: 0.90\n","Training accuracy: 95.65\n","\n","Epoch 32/1000\n","Training loss: 0.90\n","Training accuracy: 95.65\n","\n","Epoch 33/1000\n","Training loss: 0.90\n","Training accuracy: 95.65\n","\n","Epoch 34/1000\n","Training loss: 0.90\n","Training accuracy: 100.00\n","\n","Epoch 35/1000\n","Training loss: 0.90\n","Training accuracy: 95.65\n","\n","Epoch 36/1000\n","Training loss: 0.90\n","Training accuracy: 100.00\n","\n","Epoch 37/1000\n","Training loss: 0.90\n","Training accuracy: 100.00\n","\n","Epoch 38/1000\n","Training loss: 0.90\n","Training accuracy: 100.00\n","\n","Epoch 39/1000\n","Training loss: 0.90\n","Training accuracy: 100.00\n","\n","Epoch 40/1000\n","Training loss: 0.90\n","Training accuracy: 100.00\n","\n","Epoch 41/1000\n","Training loss: 0.90\n","Training accuracy: 100.00\n","\n","Total training time: 30.3 seconds\n","Margin_Confidence 0.05 % accuracy :26.633165829145728) + %\n","tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n","        0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,\n","        0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,\n","        0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n","        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1,\n","        0, 0, 0, 1, 0, 0, 1], device='cuda:0')\n","  for item in data:\n","      if any(np.array_equal(item, x) for x in activeL_classical_list):\n","          new_active_classical.append(item)\n","      else:\n","          new_active_non_classical.append(item)\n","\n","        #############################\n","        #         NEW ROUND         #\n","        #     percent = 0.1   #\n","        #############################\n","        \n","\n","        #############################\n","        #    Calculating Margin     #\n","        #############################\n","        \n","340: -8.940696716308594e-08\n","483: -2.950429916381836e-06\n","125: -4.857778549194336e-06\n","360: -6.67572021484375e-06\n","524: -8.58306884765625e-06\n","610: -1.3113021850585938e-05\n","498: -1.4513731002807617e-05\n","282: -1.4990568161010742e-05\n","501: -1.755356788635254e-05\n","84: -2.0772218704223633e-05\n","[340, 483, 125, 360, 524, 610, 498, 282, 501, 84]\n","\n","        #############################\n","        #       Begin Training      #\n","        #############################\n","        \n","Training for 1000 epochs on cuda\n","Epoch 1/1000\n","Training loss: 0.73\n","Training accuracy: 47.37\n","\n","Epoch 2/1000\n","Training loss: 0.73\n","Training accuracy: 60.53\n","\n","Epoch 3/1000\n","Training loss: 0.73\n","Training accuracy: 68.42\n","\n","Epoch 4/1000\n","Training loss: 0.73\n","Training accuracy: 73.68\n","\n","Epoch 5/1000\n","Training loss: 0.73\n","Training accuracy: 86.84\n","\n","Epoch 6/1000\n","Training loss: 0.73\n","Training accuracy: 86.84\n","\n","Epoch 7/1000\n","Training loss: 0.73\n","Training accuracy: 89.47\n","\n","Epoch 8/1000\n","Training loss: 0.73\n","Training accuracy: 92.11\n","\n","Epoch 9/1000\n","Training loss: 0.73\n","Training accuracy: 94.74\n","\n","Epoch 10/1000\n","Training loss: 0.73\n","Training accuracy: 100.00\n","\n","Epoch 11/1000\n","Training loss: 0.73\n","Training accuracy: 100.00\n","\n","Epoch 12/1000\n","Training loss: 0.73\n","Training accuracy: 100.00\n","\n","Epoch 13/1000\n","Training loss: 0.73\n","Training accuracy: 100.00\n","\n","Epoch 14/1000\n","Training loss: 0.73\n","Training accuracy: 100.00\n","\n","Epoch 15/1000\n","Training loss: 0.73\n","Training accuracy: 100.00\n","\n","Epoch 16/1000\n","Training loss: 0.73\n","Training accuracy: 100.00\n","\n","Epoch 17/1000\n","Training loss: 0.73\n","Training accuracy: 100.00\n","\n","Epoch 18/1000\n","Training loss: 0.73\n","Training accuracy: 100.00\n","\n","Epoch 19/1000\n","Training loss: 0.73\n","Training accuracy: 100.00\n","\n","Epoch 20/1000\n","Training loss: 0.73\n","Training accuracy: 100.00\n","\n","Epoch 21/1000\n","Training loss: 0.73\n","Training accuracy: 100.00\n","\n","Epoch 22/1000\n","Training loss: 0.73\n","Training accuracy: 100.00\n","\n","Epoch 23/1000\n","Training loss: 0.73\n","Training accuracy: 100.00\n","\n","Epoch 24/1000\n","Training loss: 0.73\n","Training accuracy: 100.00\n","\n","Epoch 25/1000\n","Training loss: 0.73\n","Training accuracy: 100.00\n","\n","Epoch 26/1000\n","Training loss: 0.73\n","Training accuracy: 100.00\n","\n","Epoch 27/1000\n","Training loss: 0.73\n","Training accuracy: 100.00\n","\n","Epoch 28/1000\n","Training loss: 0.73\n","Training accuracy: 100.00\n","\n","Epoch 29/1000\n","Training loss: 0.73\n","Training accuracy: 100.00\n","\n","Epoch 30/1000\n","Training loss: 0.73\n","Training accuracy: 100.00\n","\n","Epoch 31/1000\n","Training loss: 0.73\n","Training accuracy: 100.00\n","\n","Epoch 32/1000\n","Training loss: 0.73\n","Training accuracy: 100.00\n","\n","Epoch 33/1000\n","Training loss: 0.73\n","Training accuracy: 100.00\n","\n","Epoch 34/1000\n","Training loss: 0.73\n","Training accuracy: 100.00\n","\n","Total training time: 30.2 seconds\n","Margin_Confidence 0.1 % accuracy :79.39698492462311) + %\n","tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n","        1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,\n","        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,\n","        1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n","  for item in data:\n","      if any(np.array_equal(item, x) for x in activeL_classical_list):\n","          new_active_classical.append(item)\n","      else:\n","          new_active_non_classical.append(item)\n","\n","        #############################\n","        #         NEW ROUND         #\n","        #     percent = 0.2   #\n","        #############################\n","        \n","\n","        #############################\n","        #    Calculating Margin     #\n","        #############################\n","        \n","71: -0.0001728832721710205\n","541: -0.00019407272338867188\n","3: -0.0001951754093170166\n","60: -0.0002250969409942627\n","377: -0.00028139352798461914\n","517: -0.00036346912384033203\n","42: -0.00037553906440734863\n","314: -0.0004139244556427002\n","19: -0.0005168914794921875\n","45: -0.0005241632461547852\n","[71, 541, 3, 60, 377, 517, 42, 314, 19, 45]\n","\n","        #############################\n","        #       Begin Training      #\n","        #############################\n","        \n","Training for 1000 epochs on cuda\n","Epoch 1/1000\n","Training loss: 0.77\n","Training accuracy: 65.28\n","\n","Epoch 2/1000\n","Training loss: 0.77\n","Training accuracy: 73.61\n","\n","Epoch 3/1000\n","Training loss: 0.77\n","Training accuracy: 87.50\n","\n","Epoch 4/1000\n","Training loss: 0.77\n","Training accuracy: 83.33\n","\n","Epoch 5/1000\n","Training loss: 0.77\n","Training accuracy: 93.06\n","\n","Epoch 6/1000\n","Training loss: 0.77\n","Training accuracy: 90.28\n","\n","Epoch 7/1000\n","Training loss: 0.77\n","Training accuracy: 94.44\n","\n","Epoch 8/1000\n","Training loss: 0.77\n","Training accuracy: 88.89\n","\n","Epoch 9/1000\n","Training loss: 0.77\n","Training accuracy: 95.83\n","\n","Epoch 10/1000\n","Training loss: 0.77\n","Training accuracy: 97.22\n","\n","Epoch 11/1000\n","Training loss: 0.77\n","Training accuracy: 97.22\n","\n","Epoch 12/1000\n","Training loss: 0.77\n","Training accuracy: 94.44\n","\n","Epoch 13/1000\n","Training loss: 0.77\n","Training accuracy: 95.83\n","\n","Epoch 14/1000\n","Training loss: 0.77\n","Training accuracy: 98.61\n","\n","Epoch 15/1000\n","Training loss: 0.77\n","Training accuracy: 98.61\n","\n","Epoch 16/1000\n","Training loss: 0.77\n","Training accuracy: 100.00\n","\n","Epoch 17/1000\n","Training loss: 0.77\n","Training accuracy: 98.61\n","\n","Epoch 18/1000\n","Training loss: 0.77\n","Training accuracy: 98.61\n","\n","Epoch 19/1000\n","Training loss: 0.77\n","Training accuracy: 100.00\n","\n","Epoch 20/1000\n","Training loss: 0.77\n","Training accuracy: 100.00\n","\n","Epoch 21/1000\n","Training loss: 0.77\n","Training accuracy: 98.61\n","\n","Epoch 22/1000\n","Training loss: 0.77\n","Training accuracy: 100.00\n","\n","Total training time: 31.3 seconds\n","Margin_Confidence 0.2 % accuracy :84.42211055276381) + %\n","tensor([1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n","        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n","        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,\n","        1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n","  for item in data:\n","      if any(np.array_equal(item, x) for x in activeL_classical_list):\n","          new_active_classical.append(item)\n","      else:\n","          new_active_non_classical.append(item)\n","\n","        #############################\n","        #         NEW ROUND         #\n","        #     percent = 0.5   #\n","        #############################\n","        \n","\n","        #############################\n","        #    Calculating Margin     #\n","        #############################\n","        \n","108: -0.00013199448585510254\n","0: -0.0006065666675567627\n","41: -0.0010916292667388916\n","135: -0.0011133253574371338\n","16: -0.0012339651584625244\n","265: -0.001350313425064087\n","50: -0.0014705657958984375\n","38: -0.0015650689601898193\n","15: -0.0016022920608520508\n","31: -0.001677095890045166\n","[108, 0, 41, 135, 16, 265, 50, 38, 15, 31]\n","\n","        #############################\n","        #       Begin Training      #\n","        #############################\n","        \n","Training for 1000 epochs on cuda\n","Epoch 1/1000\n","Training loss: 0.71\n","Training accuracy: 80.00\n","\n","Epoch 2/1000\n","Training loss: 0.71\n","Training accuracy: 89.74\n","\n","Epoch 3/1000\n","Training loss: 0.71\n","Training accuracy: 93.33\n","\n","Epoch 4/1000\n","Training loss: 0.71\n","Training accuracy: 93.33\n","\n","Epoch 5/1000\n","Training loss: 0.71\n","Training accuracy: 95.90\n","\n","Epoch 6/1000\n","Training loss: 0.71\n","Training accuracy: 95.90\n","\n","Epoch 7/1000\n","Training loss: 0.71\n","Training accuracy: 97.44\n","\n","Epoch 8/1000\n","Training loss: 0.71\n","Training accuracy: 97.95\n","\n","Epoch 9/1000\n","Training loss: 0.71\n","Training accuracy: 97.44\n","\n","Total training time: 32.6 seconds\n","Margin_Confidence 0.5 % accuracy :94.47236180904522) + %\n","tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 0, 1], device='cuda:0')\n","  for item in data:\n","      if any(np.array_equal(item, x) for x in activeL_classical_list):\n","          new_active_classical.append(item)\n","      else:\n","          new_active_non_classical.append(item)\n","\n","        #############################\n","        #         NEW ROUND         #\n","        #     percent = 0.7   #\n","        #############################\n","        \n","\n","        #############################\n","        #    Calculating Margin     #\n","        #############################\n","        \n","1: -0.008530497550964355\n","376: -0.01132163405418396\n","168: -0.011356174945831299\n","371: -0.011400014162063599\n","219: -0.011849075555801392\n","297: -0.011892437934875488\n","151: -0.01190003752708435\n","285: -0.011912137269973755\n","149: -0.011980444192886353\n","6: -0.012055635452270508\n","[1, 376, 168, 371, 219, 297, 151, 285, 149, 6]\n","\n","        #############################\n","        #       Begin Training      #\n","        #############################\n","        \n","Training for 1000 epochs on cuda\n","Epoch 1/1000\n","Training loss: 0.76\n","Training accuracy: 95.60\n","\n","Epoch 2/1000\n","Training loss: 0.75\n","Training accuracy: 95.60\n","\n","Epoch 3/1000\n","Training loss: 0.76\n","Training accuracy: 98.90\n","\n","Epoch 4/1000\n","Training loss: 0.75\n","Training accuracy: 98.90\n","\n","Epoch 5/1000\n","Training loss: 0.75\n","Training accuracy: 97.80\n","\n","Epoch 6/1000\n","Training loss: 0.75\n","Training accuracy: 97.80\n","\n","Epoch 7/1000\n","Training loss: 0.75\n","Training accuracy: 98.90\n","\n","Epoch 8/1000\n","Training loss: 0.75\n","Training accuracy: 98.90\n","\n","Epoch 9/1000\n","Training loss: 0.75\n","Training accuracy: 100.00\n","\n","Epoch 10/1000\n","Training loss: 0.75\n","Training accuracy: 100.00\n","\n","Epoch 11/1000\n","Training loss: 0.75\n","Training accuracy: 98.90\n","\n","Epoch 12/1000\n","Training loss: 0.75\n","Training accuracy: 100.00\n","\n","Epoch 13/1000\n","Training loss: 0.75\n","Training accuracy: 100.00\n","\n","Epoch 14/1000\n","Training loss: 0.75\n","Training accuracy: 100.00\n","\n","Epoch 15/1000\n","Training loss: 0.75\n","Training accuracy: 100.00\n","\n","Epoch 16/1000\n","Training loss: 0.75\n","Training accuracy: 100.00\n","\n","Epoch 17/1000\n","Training loss: 0.75\n","Training accuracy: 100.00\n","\n","Epoch 18/1000\n","Training loss: 0.75\n","Training accuracy: 100.00\n","\n","Epoch 19/1000\n","Training loss: 0.75\n","Training accuracy: 100.00\n","\n","Total training time: 31.3 seconds\n","Margin_Confidence 0.7 % accuracy :95.47738693467338) + %\n","tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 0, 1], device='cuda:0')\n","  for item in data:\n","      if any(np.array_equal(item, x) for x in activeL_classical_list):\n","          new_active_classical.append(item)\n","      else:\n","          new_active_non_classical.append(item)\n","\n","        #############################\n","        #         NEW ROUND         #\n","        #     percent = 1.0   #\n","        #############################\n","        \n","\n","        #############################\n","        #    Calculating Margin     #\n","        #############################\n","        \n","30: -0.017759650945663452\n","45: -0.01779875159263611\n","139: -0.017884880304336548\n","128: -0.017891764640808105\n","83: -0.017892956733703613\n","79: -0.017925620079040527\n","171: -0.017934203147888184\n","40: -0.017948150634765625\n","37: -0.017985671758651733\n","70: -0.017989158630371094\n","[30, 45, 139, 128, 83, 79, 171, 40, 37, 70]\n","\n","        #############################\n","        #       Begin Training      #\n","        #############################\n","        \n","Training for 1000 epochs on cuda\n","Epoch 1/1000\n","Training loss: 0.69\n","Training accuracy: 96.33\n","\n","Epoch 2/1000\n","Training loss: 0.69\n","Training accuracy: 96.33\n","\n","Epoch 3/1000\n","Training loss: 0.69\n","Training accuracy: 96.33\n","\n","Epoch 4/1000\n","Training loss: 0.69\n","Training accuracy: 96.33\n","\n","Epoch 5/1000\n","Training loss: 0.69\n","Training accuracy: 96.33\n","\n","Epoch 6/1000\n","Training loss: 0.69\n","Training accuracy: 96.33\n","\n","Epoch 7/1000\n","Training loss: 0.69\n","Training accuracy: 96.33\n","\n","Epoch 8/1000\n","Training loss: 0.69\n","Training accuracy: 96.33\n","\n","Epoch 9/1000\n","Training loss: 0.69\n","Training accuracy: 96.33\n","\n","Epoch 10/1000\n","Training loss: 0.69\n","Training accuracy: 96.33\n","\n","Epoch 11/1000\n","Training loss: 0.69\n","Training accuracy: 96.33\n","\n","Epoch 12/1000\n","Training loss: 0.69\n","Training accuracy: 96.33\n","\n","Epoch 13/1000\n","Training loss: 0.69\n","Training accuracy: 96.33\n","\n","Epoch 14/1000\n","Training loss: 0.69\n","Training accuracy: 96.33\n","\n","Epoch 15/1000\n","Training loss: 0.69\n","Training accuracy: 96.33\n","\n","Epoch 16/1000\n","Training loss: 0.69\n","Training accuracy: 96.33\n","\n","Total training time: 31.2 seconds\n","Margin_Confidence 1.0 % accuracy :89.9497487437186) + %\n","tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n"]},{"output_type":"execute_result","data":{"text/plain":["   Percentage                    Accuracy\n","0        0.02   (13.06532663316583, 1000)\n","1        0.05  (26.633165829145728, 1000)\n","2        0.10   (79.39698492462311, 1000)\n","3        0.20   (84.42211055276381, 1000)\n","4        0.50   (94.47236180904522, 1000)\n","5        0.70   (95.47738693467338, 1000)\n","6        1.00    (89.9497487437186, 1000)"],"text/html":["\n","  <div id=\"df-d5ac0191-58be-45fa-b6b1-f9f50e3ab502\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Percentage</th>\n","      <th>Accuracy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.02</td>\n","      <td>(13.06532663316583, 1000)</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.05</td>\n","      <td>(26.633165829145728, 1000)</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.10</td>\n","      <td>(79.39698492462311, 1000)</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.20</td>\n","      <td>(84.42211055276381, 1000)</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.50</td>\n","      <td>(94.47236180904522, 1000)</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>0.70</td>\n","      <td>(95.47738693467338, 1000)</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>1.00</td>\n","      <td>(89.9497487437186, 1000)</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d5ac0191-58be-45fa-b6b1-f9f50e3ab502')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-d5ac0191-58be-45fa-b6b1-f9f50e3ab502 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-d5ac0191-58be-45fa-b6b1-f9f50e3ab502');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-5f1489bb-98f3-4a1b-8208-5e4e4eeace88\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-5f1489bb-98f3-4a1b-8208-5e4e4eeace88')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-5f1489bb-98f3-4a1b-8208-5e4e4eeace88 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":8}]}]}